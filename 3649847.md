Verification of Neural Networks’ Global Robustness

ANAN KABAHA, Technion, Israel
DANA DRACHSLER COHEN, Technion, Israel

Neural networks are successful in various applications but are also susceptible to adversarial attacks. To show the safety of network classifiers, many verifiers have been introduced to reason about the local robustness of a given input to a given perturbation. While successful, local robustness cannot generalize to unseen inputs. Several works analyze global robustness properties, however, neither can provide a precise guarantee about the cases where a network classifier does not change its classification. In this work, we propose a new global robustness property for classifiers aiming at finding the minimal globally robust bound, which naturally extends the popular local robustness property for classifiers. We introduce VHAGaR, an anytime verifier for computing this bound. VHAGaR relies on three main ideas: encoding the problem as a mixed-integer programming and pruning the search space by identifying dependencies stemming from the perturbation or the network’s computation and generalizing adversarial attacks to unknown inputs. We evaluate VHAGaR on several datasets and classifiers and show that, given a three hour timeout, the average gap between the lower and upper bound on the minimal globally robust bound computed by VHAGaR is 1.9, while the gap of an existing global robustness verifier is 154.7. Moreover, VHAGaR is 130.6x faster than this verifier. Our results further indicate that leveraging dependencies and adversarial attacks makes VHAGaR 78.6x faster.

CCS Concepts: • Theory of computation → Program analysis; Program verification; • Software and its engineering → Formal methods; • Computing methodologies → Neural networks.

Additional Key Words and Phrases: Neural Network Verification, Global Robustness, Constrained Optimization

ACM Reference Format:
Anan Kabaha and Dana Drachsler Cohen. 2024. Verification of Neural Networks’ Global Robustness. Proc. ACM Program. Lang. 8, OOPSLA1, Article 130 (April 2024), 30 pages. https://doi.org/10.1145/3649847

1 INTRODUCTION

Deep neural networks are successful in various tasks but are also susceptible to adversarial examples: malicious input perturbations designed to deceive the network. Many adversarial attacks target image classifiers and compute either an imperceptible change, bounded by a small $\epsilon$ with respect to an $L_p$ norm (e.g., $p = 0, 1, 2, \infty$) [Carlini and Wagner. 2017; Chen et al. 2018, 2017; Madry et al. 2018; Szegedy et al. 2014; Zhang et al. 2020, 2022b], or a perceivable change obtained by perturbing a semantic feature that does not change the semantics of the input, such as brightness, translation, or rotation [Bhattad et al. 2020; Engstrom et al. 2019, 2017; Liu et al. 2019b; Wu et al. 2019].

A popular safety property for understanding a network’s robustness to such attacks is local robustness. Local robustness is parameterized by an input and its neighborhood. For a network classifier, the goal is to prove that the network classifies all inputs in the neighborhood the same. Several local robustness verifiers have been introduced for checking robustness in an $\epsilon$-ball [Qin et al. 2019; Singh et al. 2019a,b; Tjeng et al. 2019; Wang et al. 2018, 2021] or a feature neighborhood [Balunovic et al. 2019; Mohapatra et al. 2020]. However, local robustness is limited to reasoning about a single...
neighborhood at a time. Thus, the network designer has to reason separately about every input that may arise in practice. This raises several issues. First, the space of inputs typically has a high dimensionality, making it impractical to be covered by a finite set of neighborhoods. Second, the robustness of a set of neighborhoods does not imply that unseen neighborhoods are robust. Third, even if the network designer can identify a finite set of relevant neighborhoods, existing local robustness verifiers take non-negligible time to reason about a single neighborhood. Thus, running them on a very large number of neighborhoods is impractical. Hence, local robustness does not enable to fully understand the robustness level of a network to a given perturbation for all inputs.

These issues gave rise to reasoning about a network’s robustness over all possible inputs, known as global robustness. Several works analyze global robustness properties. They can be categorized into two primary groups: precise analysis [Katz et al. 2017, 2019; Wang et al. 2022a,b] and sampling-based analysis [Bastani et al. 2016; Levy et al. 2023; Mangal et al. 2019; Ruan et al. 2019]. Existing precise analysis techniques focus primarily on the $L_\infty$-ball and a global robustness property defined over the network’s stability [Katz et al. 2017; Wang et al. 2022a,b]. The network’s stability is the maximum difference of the output vectors of an input and a perturbed example in that input’s $L_\infty$-ball. This approach cannot capture the desired robustness property for classifiers, which is that the inputs’ classification does not change under a given perturbation. The second kind of analyzers computes a probabilistic global robustness bound by analyzing the local robustness of samples from the input space [Levy et al. 2023; Mangal et al. 2019] or a given dataset [Bastani et al. 2016; Ruan et al. 2019]. Sampling-based analyzers scale better than the precise analyzers but only provide a lower bound on the actual global robustness bound.

In this work, we propose a new global robustness property designated for classifiers, which is general to any perturbation such as the $L_\infty$ perturbation or feature perturbations. Intuitively, a classifier is globally robust to a given perturbation if for any input for which the network is confident enough in its classification, this perturbation does not cause the network to change its classification. We focus on such inputs because it is inevitable that inputs near the decision boundaries, where the network’s classification confidence is low, are misclassified when adding perturbations. Note that, like previous works [Katz et al. 2017, 2019; Wang et al. 2022a,b], our global robustness property considers any input, including meaningful inputs and meaningless noise. In a scenario where the focus is only on meaningful inputs, our property can theoretically be restricted to this space. However, this requires a formal characterization of the meaningful input space, which is generally nonexistent. For such scenario, our property, which considers any input, can be viewed as over-approximating the space of meaningful inputs for which the network is confident enough.

We address the problem of computing the minimal globally robust bound of a given network and perturbation. Namely, any input that the network classifies with a confidence that is at least this bound is not misclassified under this perturbation. Although our global robustness property is not the first to consider any input, it is the first to consider the classifier’s minimal bound satisfying global robustness. This difference is similar to the difference between verifying local robustness at a given $L_\infty$-ball and computing the maximal $L_\infty$-ball that is locally robust. Our problem is highly challenging for two main reasons. First, it is challenging to determine for a given confidence whether a classifier is globally robust since it requires to determine the robustness of a very large space of unknown inputs; even local robustness verification, reasoning about a single input’s neighborhood, is under active research. Second, our problem requires to compute the minimal globally robust bound, and thus it requires reasoning about the global robustness over a large number of confidences.

We introduce VHAGaR, a Verifier of Hazardous Attacks for proving Global Robustness that computes the minimal globally robust bound of a network classifier, under a given perturbation. VHAGaR relies on three key ideas. First, it encodes the problem as a mixed-integer programming
Fig. 1. (a) The perturbations supported by VHAGaR. (b) VHAGaR’s upper and lower bounds on the minimal globally robust bound vs. sampling approaches, Marabou and a MIP-only variant of VHAGaR.

(MIP), enabling efficient optimization through existing MIP solvers. Further, VHAGaR is designed as an anytime algorithm and asks the MIP solver to compute a lower and an upper bound on the minimal globally robust bound. Second, it prunes the search space by computing dependencies stemming from the perturbation or the network’s computation. Third, it executes a hyper-adversarial attack, generalizing adversarial attacks to unknown inputs, to efficiently compute suboptimal lower bounds on the robustness bound that further prune the search space. Our attack also identifies optimization hints, partial assignments to the MIP’s variables, guiding towards the optimal solution. VHAGaR currently supports the $L_\infty$ perturbation and six semantic feature perturbations (Figure 1(a)).

We evaluate VHAGaR on several fully-connected and convolutional image classifiers, trained for MNIST, Fashion-MNIST and CIFAR-10. We compare to existing approaches, including Marabou [Katz et al. 2019] and sampling-based approaches: Dataset Sampling, relying on a dataset, and Random Sampling, sampling from the input domain. We further show the importance of integrating the dependencies and hyper-adversarial attacks by comparing them to a MIP-only variant of VHAGaR. Our results show that (1) the average gap between the lower and upper bound of VHAGaR is 1.9, while Marabou’s gap is 154.7, (2) the lower bound of VHAGaR is greater (i.e., tighter) by 6.5x than Marabou, by 9.2x than Dataset Sampling and by 25.6x than Random Sampling, (3) the upper bound of VHAGaR is smaller (i.e., tighter) by 13.1x than Marabou, and (4) VHAGaR is 130.6x faster than Marabou. Our results further indicate that VHAGaR is 78.6x faster than the MIP-only variant. Figure 1(b) shows our results for an MNIST convolutional network, classifying images to digits, and an occlusion perturbation, blackening a 3x3 square at the center of the image. The goal of this experiment is to compute the minimal globally robust bound for which any input that the network classifies as the digit 2 with a confidence greater or equal to this bound cannot be perturbed by this occlusion such that the network classifies it as the digit 3. The plot shows that, given a three hour timeout, VHAGaR returns the optimal minimal bound (the lower and upper bound are equal), while the gap of the MIP-only variant is 10.1 and the gap of Marabou is 86.6. The sampling baselines, providing only a lower bound, return a bound lower than the minimal bound by 3x. Figure 1(b) highlights in a light blue background the range of confidences for which every approach proves global robustness, and in a light red background the range of confidences for which every approach guarantees there is no global robustness. VHAGaR perfectly partitions this range, while the other approaches leave a range of confidences without any guarantee. We further demonstrate how to leverage VHAGaR’s bounds to infer global robustness and vulnerability attributes of the network.

2 PRELIMINARIES

In this section, we provide background on network classifiers. We focus on image classifiers. An image is a $d_1 \times d_2$ matrix, where $d_1$ is the height and $d_2$ is the width. The entries are pixels consisting of $l$ channels, each over $[0, 1]$. If $l = 1$, the image is in grayscale, if $l = 3$, it is colored (RGB). Given an image domain $[0, 1]^{l \times d_1 \times d_2}$ and a set of classes $C = \{1, \ldots, c\}$, a classifier maps images to a score vector over the possible classes $D : [0, 1]^{l \times d_1 \times d_2} \rightarrow \mathbb{R}^c$. We assume a non-trivial classifier,
that is \( c > 1 \) and for any class \( c' \in C \) there is an input classified as \( c' \). A neural network classifier consists of an input layer followed by \( L \) layers. For simplicity’s sake, our definitions treat all layers as vectors, except where the definition requires a matrix. The input layer \( z_0 \) takes as input an image \( x \) and passes it to the next layer (i.e., \( z_{0,k} = x_k \)). The last layer outputs a vector, denoted \( D(x) \), consisting of a score for each class in \( C \). The classification of the network for input \( x \) is the class with the highest score, \( c' = \text{argmax}(D(x)) \). The layers are functions, denoted \( h_1, h_2, \ldots, h_L \), each taking as input the output of the preceding layer. The network’s function is the composition of the layers: \( D(x) = h_L(h_{L-1}(\cdots(h_1(x)))) \). A layer \( m \) consists of \( k_m \) neurons, denoted \( z_{m,1}, \ldots, z_{m,k_m} \). The function of layer \( m \) is defined by its neurons, i.e., it outputs the vector \( (z_{m,1}, \ldots, z_{m,k_m})^T \). There are several types of layers. We focus on fully-connected and convolutional layers, but our approach is easily extensible to other layers such as max pooling layers and residual layers [He et al. 2016].

In a fully-connected layer, a neuron \( z_{m,k} \) gets as input the outputs of all neurons in the preceding layer. It has a weight for each input \( w_{m,k,k'} \) and a single bias \( b_{m,k} \). Its function is computed by first computing the sum of the bias and the multiplication of every input by its respective weight:

\[
\hat{z}_{m,k} = b_{m,k} + \sum_{k'=1}^{k_m} w_{m,k,k'} \cdot z_{m-1,k'}.
\]

This output is passed to an activation function \( \sigma \) to produce the output \( z_{m,k} = \sigma(\hat{z}_{m,k}) \). Activation functions are typically non-linear functions. We focus on ReLU, defined as \( \text{ReLU}(\hat{z}) = \max(0, \hat{z}) \). A convolutional layer is similar to a fully-connected layer except that a neuron does not get as input all neurons from the preceding layer, but only a small subset of them. Additionally, neurons at the same layer share their weights and bias. The set of shared weights and bias are collectively called a kernel. Formally, a convolutional layer views its input as a matrix \( l \times d_{m-1}^h \times d_{m-1}^w \), where \( l \) is the number of input channels, \( d_{m-1}^h \) is the height and \( d_{m-1}^w \) is the width. A kernel \( K \) consists of a weight matrix \( W_m \) of dimension \( l \times l \times l \) and a bias \( b_m \). A kernel is convolved with the input \( \hat{z}_{m,l,i,j} = b_m + \sum_{l=1}^{l} \sum_{p=1}^{l} \sum_{q=1}^{l} W_m[v, p, q] \cdot z_{m-1,o,p,q} \), after which an activation function (e.g., ReLU) is executed component-wise.

### 3 Minimal Globally Robust Bounds

In this section, we present our global robustness property designated for network classifiers. We begin with a few definitions and then define our property and the problem we address.

**Perturbations.** A perturbation is a function \( f_p(x, E) \), defining how an input \( x \) is perturbed given an amount of perturbation specified by a vector \( E = (e_1, \ldots, e_q) \). The perturbation values \( e_i \) are either real numbers or integers. For example, brightness is defined by a brightness level \( e \in [-1, 1] \) and the function is \( f_b(x, e) = x' \) such that \( x_{o,p,q}' = x_{o,p,q} + e \), for \( v \in [l], p \in [d_1], q \in [d_2] \). Translation is defined by translation coordinates \( i \in [-d_1, d_1] \) and \( j \in [-d_2, d_2] \) and the function is \( f_t(x, i, j) = x' \) such that \( x_{o,p,q}' = (p-i \in [d_1] \land q-j \in [d_2]) \land x_{o,p-i,q-j} = 0 \). The \( L_\infty \) perturbation has a perturbation limit \( e \in [0, 1] \) and is defined by a series of perturbations \( e_{o,p,q} \in [-e, e] \), for \( v \in [l], p \in [d_1], q \in [d_2] \). The function is \( f_{L_\infty}(x, e_{1,1,1}, \ldots, e_{l,d_1,d_2}) = x' \) such that \( x_{o,p,q}' = x_{o,p,q} + e_{o,p,q} \). We note that if a perturbed entry is not in its valid range (i.e., below 0 or above 1), it is clipped: \( \min(\max(f_p(x, E)_{o,p,q}, 0), 1) \). We omit clipping from the formal definitions to avoid a cluttered notation. A perturbation is confined by a range \( I_E \), a series of intervals bounding each entry of \( E \). For example, a range for brightness is \( [0.1, 0.3] \subseteq [-1, 1] \), bounding the values of the brightness level \( e \), and a range for translation is \( ([1, 3], [2, 10]) \subseteq ([−d_1, d_1], [−d_2, d_2]) \) (for \( d_1 \geq 3, d_2 \geq 10 \)), bounding the coordinates \( i \) and \( j \). An amount of perturbation \( E' \) is in a given range \( I_E \), denoted \( E' \in I_E \), if its entries are in their intervals: \( \forall k. E'_k \in (I_E)_k \). For example, \( (2, 4) \in ([1, 3], [2, 10]) \).

**Local robustness.** A common approach to estimate the robustness of a classifier focuses on a finite set of inputs and checks local robustness for every input, one-by-one. Local robustness is checked at a given neighborhood, defined with respect to an input and a perturbation. Formally, given an input
x, a perturbation P and its range I_E, a neighborhood N_{P,I_E}(x) ⊆ [0, 1]^{1×d_1×d_2} is the set of all inputs obtained by this perturbation: N_{P,I_E}(x) = \{ f_P(x, E') | E' ∈ I_E \}. Given a classifier D, an input x and a neighborhood containing x, N_{P,I_E}(x) ⊆ [0, 1]^{1×d_1×d_2}, the classifier D is locally robust at N_{P,I_E}(x) if it classifies all inputs the same: ∀x' ∈ N_{P,I_E}(x). \text{argmax}(D(x')) = \text{argmax}(D(x)). Equivalently, D is locally robust at x under P and I_E if ∀E' ∈ I_E. \text{argmax}(D(f_P(x, E'))) = \text{argmax}(D(x)).

Towards a global robustness property. Given a perturbation P and its range I_E, an immediate extension of the local robustness definition to global robustness is:

∀x∀E' ∈ I_E. \text{argmax}(D(f_P(x, E'))) = \text{argmax}(D(x))

However, no (non-trivial) network classifier and non-trivial perturbation satisfy this property because some inputs x are close to the classifier’s decision boundaries [Alfarra et al. 2020; Huang 2020] and violate this property. We thus consider only inputs whose network’s confidence is high enough and thus are far enough from the decision boundaries. Formally, the network’s confidence in classifying an input x as a class c' ∈ {1, ..., c} is the difference between the score the network assigns to c' and the maximal score it assigns to any other class. If this confidence is positive, the network classifies x as c', and the higher the (positive) number, the more certain the network is in its classification of x as c'.

Definition 3.1 (Class Confidence). Given a classifier D, an input x and a class c' ∈ C, the class confidence of D in c' is C(x, c', D) ≜ D(x)_{c'} - \max_{c'' ≠ c'}(D(x)_{c''}).

δ-GLOBAL ROBUSTNESS. We define global robustness with respect to a confidence level δ. The definition restricts the above definition to inputs whose class confidence is at least δ.

Definition 3.2 (δ-Globally Robust Classifier). Given a classifier D, a class c' ∈ C, a perturbation P, a range I_E and a class confidence δ > 0, the classifier D is δ-globally robust for c' under (P, I_E) if:

∀x∀E' ∈ I_E. C(x, c', D) ≥ δ ⇒ \text{argmax}(D(f_P(x, E'))) = \text{argmax}(D(x))

The postcondition \text{argmax}(D(f_P(x, E'))) = \text{argmax}(D(x)) is equivalent to C(f_P(x, E'), c', D) > 0, whenever the precondition C(x, c', D) ≥ δ holds. We thus use these constraints interchangeably. When it is clear from the context, we write D is δ-globally robust and omit c' and (P, I_E). The parameter δ in the precondition provides the necessary flexibility. The higher the δ, the fewer inputs that are considered and the more likely that the classifier is globally robust with respect to that δ. To understand the global robustness level of a classifier, one has to compute the minimal δ for which the classifier is globally robust. This is the problem we address, which we next define.

Definition 3.3 (Minimal Globally Robust Bound). Given a classifier D, a class c', a perturbation P and a range I_E, our goal is to compute a class confidence δ_{MIN} ∈ R^+ satisfying: (1) D is δ_{MIN}-globally robust for c' under (P, I_E), and (2) for every δ < δ_{MIN}, D is not δ-globally robust for c' under (P, I_E).

This problem is highly challenging for two reasons. First, determining for a given δ whether a classifier D is δ-globally robust for c' under (P, I_E) (for non-trivial δ, D, P and I_E) requires to determine whether D classifies a very large space of inputs as c'. Second, to determine the minimal confidence δ_{MIN} for which D is δ_{MIN}-globally robust for c' under (P, I_E) requires to determine for a large set of δ values whether D is δ-globally robust for c' under (P, I_E).

Targeted global robustness. The above definitions aim to show that the network does not change its classification under a given perturbation. In some scenarios, network designers may permit some class changes. For example, they may allow the network to misclassify a perturbed dog as a cat, but not as a truck. Thus, when determining global robustness, it is sometimes more useful to
guarantee that the network does not change its classification to a target class, rather than to any class but the correct class. We next adapt our definitions to targeted global robustness.

**Definition 3.4 (δ-Targeted Globally Robust Classifier).** Given a classifier \( D \), a class \( c' \in C \), a target class \( c_t \in C \) such that \( c_t \neq c' \), a perturbation \( P \), a range \( I_E \) and a class confidence \( \delta > 0 \), the classifier \( D \) is \( \delta \)-globally robust against \( c_t \) under \((P, I_E)\) if:

\[
\forall x \forall E' \in I_E. \ C(x, c', D) \geq \delta \Rightarrow \text{argmax}(D(f_P(x, E'))) \neq c_t
\]

The problem definition of computing the minimal targeted globally robust bound for a target class \( c_t \) is the same as Definition 3.3 but with respect to Definition 3.4. In the next section, we continue with the untargeted definition (Definition 3.3), but all definitions and algorithms easily adapt to the targeted definition.

### 4 OVERVIEW OF KEY IDEAS

In this section, we present our main ideas to compute the minimal globally robust bound. We begin by introducing the straightforward optimization formalism for our problem and explaining why it is not amenable to standard MIP solvers. Then, we introduce the key components of VHAGaR. First, we rephrase the optimization problem into a form supported by MIP solvers, which also enables VHAGaR an anytime optimization (Section 4.1). The anytime computation splits the problem into computing an upper bound and a lower bound for the minimal globally robust bound. Second, we identify dependencies that reduce the time complexity of solving the optimization problem (Section 4.2). Third, we compute a suboptimal lower bound and optimization hints, using a hyper-adversarial attack, which are provided to the MIP solver to expedite the tightening of the bounds (Section 4.3). Figure 2 demonstrates the importance of our steps. This figure shows the upper and lower bounds obtained by the anytime optimization as a function of the time, for a convolutional network composed of 540 neurons trained for the MNIST dataset and an occlusion perturbation. Figure 2(a) shows the bounds obtained by our encoding (Section 4.1). While the bounds are tightened given more time, their gap is quite large after ten minutes. Figure 2(b) shows the improvement obtained by encoding the dependencies (Section 4.2): the bounds are tightened more significantly and more quickly. Figure 2(c) shows the improvement obtained by additionally providing the suboptimal lower bound and the optimization hints (Section 4.3): the lower and upper bounds are tightened more quickly and converge to the minimal globally robust bound within 72 seconds.

#### 4.1 Minimal Globally Robust Bound via Optimization

In this section, we phrase the problem of minimal globally robust bound as constrained optimization. Phrasing a problem as constrained optimization and providing it to a suitable solver has been shown to be very efficient in solving complex search problems and in particular robustness-related
tasks, such as computing adversarial examples [Carlini and Wagner. 2017; Chen et al. 2017; Madry et al. 2018; Szegedy et al. 2014], proving local robustness [Singh et al. 2019a,c; Tjeng et al. 2019; Wang et al. 2021] and computing maximally robust neighborhoods [Erdemir et al. 2022; Kabaha and Drachsler-Cohen 2023; Li et al. 2021; Liu et al. 2019a]. However, the straightforward formulation of our task, building on the aforementioned tasks, is not amenable to existing MIP solvers. We thus present an equivalent formulation, amenable to these solvers. We begin with the straightforward formulation and then show the equivalent formulation. The straightforward formulation follows directly from Definition 3.3:

**Problem 1 (Minimal Globally Robust Bound).** Given a classifier $D$, a class $c'$, a perturbation $P$ and a range $I_E$, the constrained optimization computing the minimal globally robust bound is:

$$\min \delta \quad \text{subject to} \quad \forall x \forall E' \in I_E. \ C(x, c', D) \geq \delta \Rightarrow C(f_P(x, E'), c', D) > 0$$

(1)

This constraint requires that for any input classified as $c'$ with confidence greater or equal to $\delta$, its perturbations are classified as $c'$. The optimal solution $\delta^*_1$ ranges over $[\Delta, \delta^m + \Delta]$, where $\delta^m$ is the maximal confidence that the classifier $D$ assigns to $c'$ for any input (i.e., $\delta^m = \max_x C(x, c', D)$) and $\Delta$ is a very small number that denotes the computer’s precision level for representing the real numbers. For example, if the perturbation is the identity function (i.e., $f_P(x, E') = x$), then $\delta^*_1 = \Delta$. More generally, if the perturbation does not lead $D$ to change the classification for any input, then $\delta^*_1 = \Delta$. A different example is a perturbation that can perturb every pixel within its range $[0, 1]$. In this case, $\delta^*_1 = \delta^m + \Delta$. This follows since, under this perturbation, the right-hand side of the constraint pertains to every input (i.e., for every $x, x' \in [0, 1]^{1 \times d_1 \times d_2}$ there is $E'$ such that $f_P(x, E') = x'$). Since we assume $D$ is a non-trivial classifier that has at least two classes ($c > 1$), every input $x$ violates the postcondition because there are inputs $x'$ not classified as $c'$. Namely, $\delta^m$ is the maximal value for which the postcondition does not hold. Since for $\delta^m + \Delta$ the precondition is vacuously true, $\delta^*_1 = \delta^m + \Delta$ is the minimal value satisfying the constraint of Problem 1. For other (more interesting) perturbations, $\delta^*_1 \in (\Delta, \delta^m + \Delta)$ (note that $\delta^m$ depends on $D$).

Problem 1 cannot be submitted to existing MIP solvers, because its constraint has a for-all operator defined over a continuous range. Instead, we consider the dual problem: computing the maximal globally non-robust bound. This problem looks for the maximal $\delta$ violating the constraint of Problem 1. Namely, for every $\delta' > \delta$ the network is $\delta'$-globally robust, and for every $\delta' \leq \delta$ there is an input $x$ and a perturbation amount $E'$ for which the network is not robust. Formally:

**Problem 2 (Maximal Globally Non-Robust Bound).**

$$\max \delta \quad \text{subject to} \quad \exists x \exists E' \in I_E. \ C(x, c', D) \geq \delta \land C(f_P(x, E'), c', D) \leq 0$$

(2)

In the special case where the perturbation does not lead $D$ to change its classification for any input, i.e., $\delta^*_1 = \Delta$, Problem 2 is infeasible. For completeness of the definition, in this case we define $\delta^*_2 \equiv 0$. Thus, by definition, the optimal solution $\delta^*_2$ ranges over $[0, \delta^m]$. Note that Problem 1 and Problem 2 have very close optimal solutions: their difference is the precision level, namely $\delta^*_1 - \delta^*_2 = \Delta$. This is because $\delta^*_1$ is the minimal confidence for which there is no adversarial example and $\delta^*_2$ is the maximal confidence for which there is an adversarial example. Formally:

**Lemma 4.1.** Let $\delta^*_1$ be the optimal value of Problem 1 and $\delta^*_2$ be the optimal value of Problem 2 (where $\delta^*_2 \equiv 0$ if Problem 2 is infeasible). Then $\delta^*_1 - \delta^*_2 = \Delta$, where $\Delta$ is the precision level.

**Proof.** If Problem 2 is infeasible, then $\delta^*_2 \equiv 0$. In this case, all perturbed examples are classified as $c'$ and by definition of Problem 1, $\delta^*_1 = \Delta$. If Problem 2 is feasible, then since it is bounded (by $\delta^m$), it has an optimal solution $\delta^*_2$. If we add to $\delta^*_2$ the smallest possible value $\Delta$, we obtain
\( \delta^*_2 + \Delta \), which violates the constraint of Problem 2. Since Problem 1’s constraint is the negation of Problem 2’s constraint, \( \delta^*_2 + \Delta \) satisfies the constraint of Problem 1 and is thus a feasible solution. It is also the minimal value satisfying this constraint: subtracting the smallest value possible \( \Delta \) results in \( \delta^*_2 \), which does not satisfy Problem 1’s constraint. Thus, \( \delta^*_1 = \delta^*_2 + \Delta \). \( \square \)

In the following, we sometimes abuse terminology and say that the optimal solution of Problem 2, \( \delta^*_2 \), is an optimal solution to Problem 1, because given \( \delta^*_2 \), we can obtain \( \delta^*_1 \) by adding the precision level \( \Delta \). Besides having close optimal solutions, the problems have the same time complexity, which is very high (as we shortly explain). The main difference between these problems is that the second formulation is supported by existing MIP solvers. A common and immediate practical approach to address highly complex problems is by anytime optimization. We next discuss at high-level the time complexity and how we leverage the anytime optimization.

**Problem complexity.** VHAGaR solves Problem 2 by encoding the problem as a MIP problem (the exact encoding is provided in Section 5.1) and then submitting it to a MIP solver. The encoding includes the network’s computation twice: once as part of the encoding of the class confidence of \( x \) and once as part of the encoding of the class confidence of \( f_{\mathcal{E}}(x, \mathcal{E}') \). Each network’s computation is expressed by the encoding proposed by Tjeng et al. [2019] for local robustness analysis. This encoding assigns to each non-linear computation of the network (e.g., ReLU) a boolean variable, whose domain is \( \{0, 1\} \). Generally, the time complexity of a MIP problem over boolean and real variables is exponential in the number of boolean variables. To make the analysis more efficient, Tjeng et al. [2019] identify boolean variables that can be removed (e.g., ReLUs whose inputs are non-positive or non-negative), thereby reducing the complexity. Despite the exponential complexity, MIP encoding is popular for verifying local robustness [Singh et al. 2019a,c; Tjeng et al. 2019; Wang et al. 2021]. However, for global robustness, where the network’s computation is encoded twice, the exponent is twice as large. This leads to even larger complexity, posing a significant challenge for the MIP solver. To enable the MIP solver to reach a solution for our optimization problem, we propose several steps, the first one is a built-in feature of existing solvers, and we next describe it.

**Anytime optimization.** One immediate mitigation for the high time complexity is to employ anytime optimization. We next describe how Gurobi, a state-of-the-art optimizer, supports anytime optimization. To find the optimal solution to Problem 2, \( \delta^*_2 \), Gurobi defines two bounds: an upper bound \( \delta_U \geq \delta^*_2 \) and a lower bound \( \delta_L \leq \delta^*_2 \). The upper bound is initialized by a large value, e.g., \( \delta_U = \text{MAX\_FLOAT} \), that satisfies the constraint of our original optimization problem (Problem 1). It then iteratively decreases \( \delta_U \) until reaching a value violating this constraint. For the lower bound, Gurobi looks for feasible solutions for our optimization problem (Problem 2). That is, it looks for inputs classified as \( c' \) and corresponding perturbations \( \mathcal{E}' \) that cause the network to misclassify. These parallel updates continue until \( \delta_U = \delta_L \). This modus operandi provides a natural anytime optimization: at any time the user can terminate the optimization and obtain that \( \delta^*_2 \in [\delta_L, \delta_U] \). That is, an anytime optimization provides a practical approach to bound the network’s maximal globally non-robust bound (and hence bound the minimal globally robust bound \( \delta^*_1 \in [\delta_L + \Delta, \delta_U + \Delta] \)). Combined with our other steps to scale the optimization, VHAGaR often reaches relatively tight bounds (i.e., the difference \( \delta_U - \delta_L \) is small), as we show in Section 6.

### 4.2 Reducing Complexity via Encoding of Dependencies

In this section, we explain our main idea for reducing the time complexity of our optimization problem: encoding dependencies stemming from the perturbation or the network’s computation.
This step expedites the time to tighten the bounds of the optimal solution. Recall that the optimization problem has a constraint over the class confidence of an input $C(x, c', D)$ and the class confidence of its perturbed example $C(f_p(x, E'), c', D)$. The perturbation function $f_p$ imposes relations between the input $x$ and the perturbed input $f_p(x, E')$. For some perturbations, these relations can be encoded as linear or quadratic constraints. Additionally, since our encoding captures the network’s computation for an input and its perturbed example, the outputs of respective neurons may also be linearly dependent via equality or inequality constraints. Adding these dependencies to our MIP encoding can significantly reduce the problem’s complexity, as we next explain.

As described, the time complexity is governed by the number of boolean variables in the MIP encoding. VHAGaR relies on the encoding by Tjeng et al. [2019] in which every neuron has a unique real-valued variable and every non-input neuron (which executes ReLU) has a unique boolean variable. Since VHAGaR encodes the network twice, once for the input and once for the perturbed example, every neuron has two real-valued variables and two boolean variables. The real-valued variables of neuron $k$ in layer $m$ are $z_{m,k}$ for the input and $z^p_{m,k}$ for the perturbed example, and they capture the computation defined in Section 2. The boolean variables are $a_{m,k}$ for the input and $a^p_{m,k}$ for the perturbed example. If $a_{m,k} = 1$ (respectively $a^p_{m,k} = 1$), then this neuron’s ReLU is active, $z_{m,k} \geq 0$ (respectively $z^p_{m,k} \geq 0$); otherwise, it is inactive, $z_{m,k} \leq 0$ (respectively $z^p_{m,k} \leq 0$). Several works propose ways to reduce the number of boolean variables [Ehlers 2017; Singh et al. 2019c; Wang et al. 2018, 2022a]. Two common approaches are: (1) computing the concrete lower and upper bounds of ReLU neurons, by submitting additional optimization problems to the MIP solver, to identify ReLU computations that are in a stable state (i.e., their inputs are non-positive or non-negative) and (2) over-approximating the non-linear ReLU computation using linear constraints. We show how to compute equality and inequality constraints over respective variables ($z_{m,k}, z^p_{m,k}$ and $a_{m,k}, a^p_{m,k}$), stemming from the network’s computation, via additional optimization problems (described in Section 5.2). Additionally, we identify dependencies that stem from the perturbation and do not involve additional optimization, as we next explain.

Many perturbations impose relations expressible as linear or quadratic constraints over the input and its perturbed example. These constraints allow pruning the search space. In particular, sometimes the dependencies pose an equality constraint over two boolean variables, effectively reducing the number of boolean variables, and thereby reducing the problem’s complexity. We define the perturbation dependencies formally in Section 5.2.2, after describing the MIP encoding. We next demonstrate at a high-level the perturbation dependency of the occlusion perturbation.

**Example.** Consider Figure 3, showing an example of a small network comprising 16 inputs $z_{0,i,j} \in [0, 1]$, two outputs $o_1, o_2$, and two hidden layers. The first hidden layer is a convolutional layer composed from four ReLU neurons. The second hidden layer is a fully-connected layer composed from two ReLU neurons; its weights are depicted by the edges and its biases by the neurons. Consider the occlusion perturbation, blackening the pixel (1,1). Blackening it means setting its value to zero (i.e., the minimal pixel value). The figure shows, at the top, the input network, receiving the input, and, at the bottom, the perturbation network, receiving the perturbed example. By the encoding of Tjeng et al. [2019], each neuron of the input network has a real-valued variable, denoted $z_{m,i,j}$ for $m \in \{0, 1\}$ (for the input layer and the convolutional layer), or $z_{m,k}$ for $m = 2$ (for the fully-connected layer), where $i$, $j$, and $k$ are the indices within the layers. Each non-input neuron also has a boolean variable, denoted $a_{1,i,j}$ or $a_{2,k}$. Similarly, each neuron of the perturbation network has a real-valued variable, denoted $z^p_{m,i,j}$ for $m \in \{0, 1\}$ and $z^p_{2,k}$, and each non-input neuron has a boolean variable, denoted $a^p_{1,i,j}$ or $a^p_{2,k}$. Because the occlusion perturbation changes a single
input pixel, all input neurons besides \((1, 1)\) are the same: \(z_{0,i,j} = z_{0,i,j}^p\) for \((i, j) \in [4] \times [4] \setminus \{(1, 1)\}\). Consequently, the computations of the subsequent layers are also related. In the convolutional layer, the neurons at indices \((1, 2), (2, 1), (2, 2)\) accept the same values in the input and the perturbed networks, and the neuron at index \((1,1)\) accepts smaller or equal values in the perturbed network compared to the input network (recall \(z_{0,1,1}^p = 0 \leq z_{0,1,1}\)). Thus, VHAGaR adds the dependencies: \(z_{1,i,j} = z_{1,i,j}^p\) and \(a_{1,i,j} = a_{1,i,j}^p\) for \((i, j) \in \{(1, 2), (2, 1), (2, 2)\}\) as well as \(z_{1,1,1} \geq z_{1,1,1}^p\) and \(a_{1,1,1} \geq a_{1,1,1}^p\). Similarly, in the fully-connected layer, VHAGaR adds the dependencies \(z_{2,1} \geq z_{2,1}^p\) and \(a_{2,1} \geq a_{2,1}^p\), for the first neuron, and \(z_{2,2} \leq z_{2,2}^p\) and \(a_{2,2} \leq a_{2,2}^p\), for the second neuron. Adding these constraints to the MIP encoding prunes the search space. Further, it effectively removes three boolean variables.

### 4.3 Computing a Suboptimal Lower Bound by a Hyper-Adversarial Attack

In this section, we present our approach to expedite the tightening of the lower bound by efficiently computing a suboptimal feasible solution to Problem 2 and providing it to the MIP solver. Computing feasible solutions amounts to computing adversarial examples. This is because feasible solutions are inputs \(x\) that have a perturbation amount \(\mathcal{E}'\) that leads the network to change its classification. We present a hyper-adversarial attack, generalizing existing adversarial attacks to unknown inputs. A hyper-adversarial attack leverages numerical optimization, which is more effective for finding adversarial examples, and thereby it expedites the lower bound computation of the general-purpose MIP solver. We also use the hyper-adversarial attack to provide the MIP solver hints (a partial assignment) for the optimization variables to further guide the MIP solver to the optimal solution.

An adversarial attack gets an input and computes a perturbation that misleads the network. In our global robustness setting, there is no particular input to attack. Instead, the space of feasible solutions (i.e., inputs) is defined by the constraint: \(\exists x \exists \mathcal{E}' \in I_E. \mathcal{C}(x, c', D) \geq \delta \land \mathcal{C}(f_D(x, \mathcal{E}'), c', D) \leq 0\). A hyper-adversarial attack begins by generating a hyper-input. A hyper-input is a set of \(M\) inputs \(X = \{x_1, x_2, x_3, ..., x_M\}\) classified as \(c'\). It consists of inputs with a wide range of class confidences. This property enables the hyper-adversarial attack to quickly converge to a non-trivial suboptimal (or optimal) lower bound. This is because the large variance in the confidences means that some inputs are closer to the optimal lower bound. These inputs provide the optimizer with a good starting point to begin searching for the optimal value without knowing the optimal value. We shortly exemplify this property (Figure 4(b)). Given a hyper-input \(X\), a hyper-adversarial attack simultaneously looks for inputs that have adversarial examples and their class confidence is higher.
than those of the inputs in $X$. Unlike existing adversarial attacks, a hyper-adversarial attack does not look for a perturbation that causes misclassification but rather for inputs that have perturbations that cause misclassification. Formally, given a hyper-input $X$, the hyper-adversarial attack is defined over input variables $\tilde{X} = (\tilde{x}_1, \ldots, \tilde{x}_M)^T$ and perturbation variables $\tilde{E}' = (\tilde{E}'_1, \ldots, \tilde{E}'_M)^T$, each corresponding to an input in $X$. The goal of the search is to find a vector $\tilde{X}$ that when added to the vector $X$ leads to at least one input with a higher class confidence that has an adversarial example. The corresponding optimization problem is the following multi-objective optimization problem:

$$\max_{\tilde{X}} \mathcal{C}(x_1 + \tilde{x}_1, c', D), \ldots, \mathcal{C}(x_M + \tilde{x}_M, c', D) \quad \text{subject to} \quad \exists \tilde{E}'_i, \mathcal{C}(f_p(x_i + \tilde{x}_i, \tilde{E}'_i), c', D) \leq 0$$  \hspace{1cm} (3)

Since all inputs in $X$ have a positive class confidence for $c'$, we can replace the $M$ maximization objectives with a single equivalent objective defined as the sum over the $M$ individual objectives. By aggregating the objectives with a sum, we streamline the optimization process. To make this constrained optimization amenable to standard optimization approaches, we strengthen the constraint and replace the disjunction by a conjunction. To solve the constrained optimization, we employ a standard relaxation (e.g., [Carlini and Wagner. 2017; Chen et al. 2017; Szegedy et al. 2014; Tu et al. 2019]) and transform it into an unconstrained optimization by presenting $M$ optimization values $\lambda = \{\lambda_1, \ldots, \lambda_M\}$ and adding the conjuncts as additional terms to the optimization goal. Overall, VHAGaR optimizes the following loss function:

$$\max_{\tilde{X}, \tilde{E}'} \sum_{i=1}^{M} \mathcal{C}(x_i + \tilde{x}_i, c', D) - \sum_{i=1}^{M} \lambda_i \cdot \mathcal{C}(f_p(x_i + \tilde{x}_i, \tilde{E}'_i), c', D)$$

VHAGaR maximizes this loss by adapting the PGD attack [Madry et al. 2018] to consider adaptive $\lambda_i$ (explained in Section 5.3). A solution to this optimization is an assignment to the variables, from which VHAGaR obtains a set of solutions denoted as $y = (y_1, \ldots, y_M)$, where $y_i = x_i + \tilde{x}_i$. VHAGaR looks for the input $y^* \in y$ maximizing the class confidence. It then provides its confidence to the MIP solver as a lower bound for our optimization problem (Problem 2). Additionally, it provides the MIP solver optimization hints. A hint is an assignment to an optimization variable from which the MIP solver proceeds its search. VHAGaR computes hints for the boolean variables (i.e., $a_{m,k}, d_{m,k}^0$), which play a significant role in the problem’s complexity. It does so by first running the feasible solutions and their perturbed examples through the network to obtain their assignment to the optimization variables. Then, it defines a hint for every boolean that has the same value for the majority of the feasible solutions. We rely on the majority to avoid biasing the optimization to a single feasible solution’s values. Next, we provide an example of our hyper-adversarial attack.

**Example.** Figure 4(a) shows a small network, which has 16 inputs $z_{0,i,j} \in [0, 1]$, two outputs $o_1, o_2$ defining two classes $C = \{c_1, c_2\}$, and two hidden layers. The weights are shown by the edges and the biases by the neurons. We consider an occlusion perturbation that blackens the pixel (1,1). For simplicity’s sake, we consider a constant perturbation, which allows us to omit the perturbation variables $\tilde{E}'$. In this example, the hyper-adversarial attack looks for inputs maximizing the class confidence of $c' = c_1$, whose perturbed examples are misclassified as $c_2$. VHAGaR begins from a hyper-input consisting of six inputs $X = (x_1, \ldots, x_6)$ classified as $c_1$ with the confidences $\{1, \ldots, 6\}$, respectively (Figure 4(a), left). Note that these inputs’ confidences are uniformly distributed over the range of confidences $[1, 6]$, providing a wide range of confidences. VHAGaR then initializes for every input a respective variable $\tilde{X} = (\tilde{x}_1, \ldots, \tilde{x}_6)$. Initially, $\tilde{x}_i = 0$, for every $i$. VHAGaR then runs the PGD attack over the loss: $\max_{\tilde{X}} \sum_{i=1}^{6} \mathcal{C}(x_i + \tilde{x}_i, c', D) - \sum_{i=1}^{6} \lambda_i \mathcal{C}(f_p(x_i + \tilde{x}_i, \tilde{E}'), c', D)$, where $f_p$ and $\tilde{E}'$ capture blackening the pixel (1,1). At a high-level, this attack runs iterations. An iteration
begins by running each input $x_i + \tilde{x}_i$ through the network to compute $C(x_i + \tilde{x}_i, c', D)$, and each $f_p(x_i + \tilde{x}_i, E')$, which is identical to $x_i + \tilde{x}_i$ except for the black pixel, through the network to compute $C(f_p(x_i + \tilde{x}_i, E'), c', D)$. Then, VHAGaR computes the gradient of the loss, updates $\bar{X}$ based on the gradient direction, and begins a new iteration, until it converges. On the right, the figure shows the inputs at the end of the optimization. In this example, all the respective perturbed examples are classified as $c_2$ and the maximal class confidence of $c_1$ is 2. The value 2 is submitted to the MIP solver as a lower bound on the maximal globally non-robust bound. This lower bound prunes for the MIP solver many exploration paths leading to inputs with class confidence lower than 2. For example, it prunes every exploration path in which $a_{1,i,j} = 0$, for every $i$ and $j$. Thereby, it prunes every assignment in the following space: $\forall i \forall j. a_{1,i,j} = 0 \land \forall i. a_{2,i} \in \{0, 1\} \land \forall i \forall j. a_{p,i,j} \in \{0, 1\} \land \forall i. a_{d,i,j} \in \{0, 1\}$. This pruning is obtained by bound propagation, a common technique in modern solvers. In addition to the lower bound, VHAGaR computes optimization hints by looking for boolean variables with the same assignment, for the majority of inputs or their perturbed examples. In this example, the hints are $\forall i \forall j. a_{1,i,j} = a_{p,i,j} = 1$ and $\forall i. a_{2,i} = a_{d,i,j} = 1$. These hints are identical to the boolean values corresponding to the optimal solution of Problem 2. By providing the hints to the MIP solver, it converges efficiently to a suboptimal (or optimal) solution.

Figure 4(b) shows the change in the class confidences $C(x_i + \tilde{x}_i, c_1, D)$ (left) and $C(f_p(x_i + \tilde{x}_i, E'), c_2, D) = -C(f_p(x_i + \tilde{x}_i, E'), c_1, D)$ (right) during the optimization, for all six inputs. The left plot shows that initially, since $\tilde{x}_i = 0$, the confidences of $x_i + \tilde{x}_i$ are uniformly distributed. As the optimization progresses, the $\tilde{x}_i$-s are updated and eventually all $x_i + \tilde{x}_i$ converge to the optimal maximal globally non-robust bound, which is 2. Note that inputs that initially have close confidences to the maximal globally non-robust bound converge faster than inputs that initially have a confidence that is farther from the bound. This demonstrates the importance of running the hyper-adversarial attack over inputs with a wide range of confidences: it enables to expedite the search for a feasible solution with a high confidence (we remind that the optimal bound is unknown).
to the optimizer and it may terminate before converging to all inputs). In particular, it increases the likelihood of reaching a suboptimal lower bound. The right plot shows that all perturbed examples are eventually classified as $c_2$.

5 VHAGAR: A VERIFIER OF HAZARDOUS ATTACKS FOR GLOBAL ROBUSTNESS

In this section, we present VHAGaR, our anytime system for computing an interval bounding the minimal globally robust bound. Figure 5 illustrates its operation. VHAGaR takes as arguments a classifier $D$, a class $c'$, a set of target classes $C_t \subseteq C \setminus \{c'\}$, a perturbation function $f_P$, and a perturbation range $I_E$. Note that VHAGaR takes as input a set of target classes, thereby it can support the untargeted definition (where $C_t = C \setminus \{c'\}$), the (single) targeted definition (where $C_t = \{c_t\}$ for some $c_t \neq c'$), or any definition in between (where $C_t$ contains several classes). VHAGaR begins by encoding Problem 2 as a MIP. This encoding consists of two copies of the network’s computation, $D$ for the input $x$ and $D^P$ for the perturbed example $x^P$. To scale the MIP solver’s computation, VHAGaR computes real-valued lower and upper bounds by adapting the approach of Tjeng et al. [2019]. VHAGaR then adds a constraint $\phi_{in}$ capturing the dependencies stemming from the perturbation and a constraint $\phi_{dep}$ capturing dependencies over the two networks’ neurons. In parallel to the MIP encoding, VHAGaR computes for each class in $C_t$ a suboptimal lower bound $\delta^{HA}$ by running the hyper-adversarial attack. From the attack, it also obtains optimization hints and passes them to the MIP solver in two matrices, $H$ for $D$ and $H^P$ for $D^P$. We note that in our implementation, the MIP encoding runs on CPUs, and in parallel the hyper-adversarial attack runs on GPUs. After the MIP encoding and the hyper-adversarial attack complete, VHAGaR submits for every target class in $C_t$ a MIP to the MIP solver (in parallel). The solver returns the lower and upper bound for each MIP, computed given a timeout. The maximum lower and upper bounds over these bounds form the interval of the maximal globally non-robust bound. VHAGaR adds the precision level $\Delta$ and returns this interval, bounding the minimal globally robust bound. We next present the MIP encoding (Section 5.1), the dependency computation (Section 5.2), and the hyper-adversarial attack (Section 5.3). Finally, we discuss correctness and running time (Section 5.4).
5.1 The MIP Encoding

In this section, we present VHAGaR’s MIP encoding which adapts the encoding of Tjeng et al. [2019] for local robustness verification. To understand VHAGaR’s encoding, we first provide background on encoding local robustness as a MIP, for a fully-connected classifier (the extension to a convolutional network is similar). Then, we describe our adaptations for our global robustness setting.

**Background.** Tjeng et al. [2019] propose a sound and complete local robustness analysis by encoding this task as a MIP problem. A classifier $D$ is locally robust at a neighborhood of an input $x$ if all inputs in the neighborhood are classified as some class $c'$. Tjeng et al. [2019]’s verifier gets as arguments a classifier $D$, an input $x$ classified as $c'$, and a neighborhood $N(x)$ defined by a series of intervals bounding each input entry, i.e., $x' \in N(x)$ if and only if $\forall k. x'_k \in [l_k, u_k]$, where $k$ is the $k$th input entry. The verifier generates a MIP for every target class $c_i \neq c'$. If there is no solution to the MIP of $c_i$, the score of $c_i$ is lower than the score of $c'$ for all inputs in $N(x)$; otherwise, there is an input whose score of $c_i$ is greater or equal to the score of $c'$. If no MIP has a solution, the network is locally robust at $N(x)$. The verifier encodes the local robustness analysis as follows.

It introduces a real-valued variable for every input neuron, denoted $z_{0,k}$. The input neurons are subjected to the neighborhood’s bounds: $\forall k. l_k \leq z_{0,k} \leq u_k$. Each internal neuron has two real-valued variables $\hat{z}_{m,k}$, for the affine computation, and $z_{m,k}$, for the ReLU computation. The affine computation is straightforward: $\hat{z}_{m,k} = b_{m,k} + \sum_{k'=1}^{k-1} w_{m,k,k'} \cdot z_{m-1,k'}$. For the ReLU computation, the verifier introduces a boolean variable $a_{m,k}$ and two concrete (real-valued) lower and upper bounds $l_{m,k}, u_{m,k} \in \mathbb{R}$, bounding the possible values of the input $\hat{z}_{m,k}$. The verifier encodes ReLU by the constraints: $z_{m,k} \geq 0$, $z_{m,k} \geq \hat{z}_{m,k}$, $z_{m,k} \leq u_{m,k} \cdot a_{m,k}$, and $z_{m,k} \leq \hat{z}_{m,k} - l_{m,k} (1 - a_{m,k})$. The concrete bounds are computed by solving the optimization problems $l_{m,k} = \min z_{m,k}$ and $u_{m,k} = \max z_{m,k}$ where the constraints are the encodings of all previous layers. We denote the set of constraints pertaining to the neurons by $D$. To determine local robustness, the verifier duplicates the above constraints $|C| - 1$ times, once for every class $c_i \neq c'$. For every $c_i$, the verifier adds the constraint $z_{L,c_i} - z_{L,c'} \geq 0$ where $L$ is the output layer. This constraint checks whether there is an input in the neighborhood whose $c_i$’s score is greater or equal to $c'$’s score. If the constraints of the MIP of $c_i$ are satisfied, the neighborhood has an input not classified as $c'$ and hence it is not locally robust. Otherwise, no input in the neighborhood is classified as $c_i$. Overall, if at least one of the $|C| - 1$ MIPs is satisfied, the neighborhood is not locally robust; otherwise, it is locally robust.

**Adaptations.** VHAGaR builds on the above encoding to compute the maximal globally non-robust bound. We next describe our adaptations supporting (1) analysis of global robustness over any input and its perturbed example, (2) encoding dependencies between the MIP variables, and (3) encoding the globally non-robust bound $\delta_{c',c_i}$ and its lower bound $\delta_{c',c_i}^A$.

To support global robustness analysis, VHAGaR has to consider any input and its perturbed example. Thus, VHAGaR has two copies of the above variables and constraints. The first copy is used for the input, while the second copy is used for the perturbed example, and its variables are denoted with a superscript $p$. We denote the set of constraints of the network propagating the perturbed example by $D^p$. To capture $x^p = f_p(x, E')$, VHAGaR adds an input constraint $\phi_{in}$ (defined in Section 5.2.2). VHAGaR encodes $E' \in I_E$, limiting the amount of perturbation, by a constraint bounding each entry of $E'$ in its interval in $I_E$. Additionally, VHAGaR computes dependencies between the MIP variables. These dependencies emerge from the input dependencies, the perturbation range $I_E$, and the network’s computation. The dependencies enable VHAGaR to reduce the problem’s complexity without affecting the soundness and completeness of the encoding. The dependency constraint $\phi_{dep}$ involves running additional MIPs (described in Section 5.2).
To encode the maximization of the globally non-robust bound $\delta$, VHAGaR performs the following adaptations. First, for every target class $c_t$, it introduces a variable $\delta_{c_t}$, capturing the maximal globally non-robust bound targeting $c_t$. Namely, it is the maximal class confidence of $c'$ over all inputs that have a perturbed example that is misclassified as $c_t$. Accordingly, it adds as an objective: max $\delta_{c_t}$. Note that by adding this objective, VHAGaR can solve the MIP in an anytime manner, because it looks for a number (the bound) and not a yes/no answer (like local robustness). Second, it encodes the constraint $C(x, c', D) \geq \delta$ by replacing the constraint $z_{L,c_t} - z_{L,c'} \geq 0$ with constraints requiring that the class confidence of $c'$ is at least $\delta_{c_t}$: $\forall c'' \in C \setminus \{c\}$. $z_{L,c'} - z_{L,c''} \geq \delta_{c_t}$. Third, it encodes the constraint $C(f_p(x, E'), c_t, D) > 0$ (the targeted version of the constraint $C(f_p(x, E'), c', D) \leq 0$). This constraint is captured by the input constraint $\phi_{in}$ and the constraints: $\forall c'' \in C \setminus \{c\}, z_{L,c_t}^p - z_{L,c''}^p > 0$ (instead of the constraint $z_{L,c_t} - z_{L,c'} \geq 0$). In case $C_t = C \setminus \{c\}$, the last constraints are defined over $\geq$ instead of $>$. Lastly, VHAGaR adds the constraint $\delta_{c_t} \geq \delta_{HA,c_t}$, where $\delta_{HA,c_t}$ is the lower bound computed by the hyper-adversarial attack. To conclude, VHAGaR submits to the MIP solver the following maximization problem for each $c_t$:

$$\max \delta_{c_t} \quad \text{subject to}$$

$$E' \in I_E; \quad \phi_{in}; \quad \phi_{dep}; \quad \delta_{c_t} \geq \delta_{HA,c_t}; \quad \forall c'' \neq c', z_{L,c'} - z_{L,c''} \geq \delta_{c_t}; \quad \forall c'' \neq c_t, z_{L,c_t}^p - z_{L,c''}^p > 0$$

$$\forall m \forall k : \quad \hat{z}_{m,k} = b_{m,k} + \sum_{k'=1}^{k-1} w_{m,k,k'} \cdot z_{m-1,k'}; \quad z_{m,k}^p = b_{m,k} + \sum_{k'=1}^{k-1} w_{m,k,k'} \cdot z_{m-1,k'}$$

$$z_{m,k} \geq 0; \quad z_{m,k} \geq \hat{z}_{m,k}; \quad z_{m,k} \leq u_{m,k} \cdot a_{m,k}; \quad z_{m,k} \leq \hat{z}_{m,k} - l_{m,k} (1 - a_{m,k})$$

$$z_{m,k}^p \geq 0; \quad z_{m,k}^p \geq \hat{z}_{m,k}^p; \quad z_{m,k}^p \leq u_{m,k}^p \cdot a_{m,k}^p; \quad z_{m,k}^p \leq \hat{z}_{m,k}^p - l_{m,k}^p (1 - a_{m,k}^p)$$

(4)

From the above encoding, we get the next two lemmas.

**Lemma 5.1.** An optimal solution to the above MIP $\delta_{c_t}$ is the maximal globally non-robust bound targeting $c_t$. Consequently, $\delta^* = \max_{c_t \in C_t} \delta_{c_t}$ is the maximal globally non-robust bound of $C_t$.

**Lemma 5.2.** The minimal globally robust bound is $\delta = \delta^* + \Delta$, where $\Delta > 0$ is the precision level.

### 5.2 Leveraging Dependencies to Reduce the Complexity

In this section, we explain how to identify dependencies between the MIP’s variables that enable to reduce the complexity of the MIP problem. VHAGaR incorporates the dependencies via the constraints $\phi_{in}$, for dependencies between the input and its perturbed example, and $\phi_{dep}$ for dependencies between neurons of the two network copies. Input dependencies are defined by the perturbation. Neuron dependencies are computed by VHAGaR from: the neurons’ concrete bounds, dependency propagation from preceding layers, or MIPs. We focus on input constraints $\phi_{in}$ defined by linear or quadratic constraints and dependency constraints $\phi_{dep}$ defined by equalities or inequalities over the MIP variables, all are supported by standard MIP solvers.

#### 5.2.1 Dependency Constraint

We begin with describing the dependency constraint $\phi_{dep}$. The goal is to identify pairs of neurons $z_{m,k}$ and $z_{m',k'}$, one from the input’s network and the other one from the perturbed example’s network, that satisfy equality or inequality constraint: $z_{m,k} \preceq z_{m',k'}$, where $\preceq \in \{\leq, <, \geq, >, =\}$. These constraints prune the search space. To determine whether $z_{m,k} \preceq z_{m',k'}$, for $\preceq \in \{\leq, <, \geq, >, =\}$, one can solve a maximization and a minimization problems, each is over the constraints of Problem 4 up to layers $m$ and $m'$, $\phi_{in}$, and the current $\phi_{dep}$ where the objectives are:

$$u_{m,k,m',k'} = \max z_{m,k} - z_{m',k'}; \quad l_{m,k,m',k'} = \min z_{m,k} - z_{m',k'}$$
The values of \( u_{m,k,m',k'} \) and \( l_{m,k,m',k'} \) determine the relation between \( z_{m,k} \) and \( z_{m',k'}^p \), as follows:

**Lemma 5.3.** Given two variables, \( z_{m,k} \) and \( z_{m',k'}^p \), and their boolean variables \( a_{m,k} \) and \( a_{m',k'}^p \):

- if \( u_{m,k,m',k'} = l_{m,k,m',k'} = 0 \), then \( z_{m,k} = z_{m',k'}^p \) and \( a_{m,k} = a_{m',k'}^p \).
- if \( l_{m,k,m',k'} \not\subseteq 0 \), then \( z_{m,k} \not\subseteq z_{m',k'}^p \) and \( a_{m,k} \not\subseteq a_{m',k'}^p \), where \( \not\subseteq \in \{ \geq, > \} \).
- if \( u_{m,k,m',k'} \not\subseteq 0 \), then \( z_{m,k} \not\subseteq z_{m',k'}^p \) and \( a_{m,k} \not\subseteq a_{m',k'}^p \), where \( \not\subseteq \in \{ \leq, < \} \).

The proof follows from the MIPs and the monotonicity of ReLU. Solving these maximization and minimization problems for all combinations of \((m, k)\) and \((m', k')\) would significantly increase the complexity of VHAGaR and is thus impractical. To cope, VHAGaR focuses on two kinds of dependencies: those stemming directly from the perturbation and those stemming from the computation of corresponding neurons (i.e., \( m' = m \) and \( k' = k \)). For the first kind of dependencies, VHAGaR adds the input dependencies (Section 5.2.2) and employs dependency propagation (defined shortly), neither requires solving the above optimization problems. For the second kind of dependencies, VHAGaR first checks the concrete bounds: if the lower bound of \( z_{m,k} \) is greater than the upper bound of \( z_{m',k'}^p \), then \( z_{m,k} > z_{m',k'}^p \) and vice-versa. Otherwise, VHAGaR computes the dependencies by solving the minimization and maximization problems (only for corresponding neuron pairs).

We next define dependency propagation. A dependency is propagated for a pair of neurons in the same layer \( z_{m,k} \) and \( z_{m',k'}^p \) if their inputs share dependencies that are preserved or reversed for \( z_{m,k} \) and \( z_{m',k'}^p \), because of their linear transformations and the ReLU definition:

**Lemma 5.4 (Dependency Propagation).** Given a neuron \( z_{m,k} \) and a neuron \( z_{m',k'}^p \), then:

- if \( \forall i \ (w_{m,k,i} \cdot z_{m-1,k_i} = w_{m',k',i} \cdot z_{m-1,k'_i}^p) \), then \( z_{m,k} = z_{m',k'}^p \) and \( a_{m,k} = a_{m',k'}^p \).
- if \( \forall i \ (w_{m,k,i} \cdot z_{m-1,k_i} \geq w_{m',k',i} \cdot z_{m-1,k'_i}^p) \), then \( z_{m,k} \geq z_{m',k'}^p \) and \( a_{m,k} \geq a_{m',k'}^p \).
- if \( \forall i \ (w_{m,k,i} \cdot z_{m-1,k_i} \leq w_{m',k',i} \cdot z_{m-1,k'_i}^p) \), then \( z_{m,k} \leq z_{m',k'}^p \) and \( a_{m,k} \leq a_{m',k'}^p \).

VHAGaR generally uses the lemma for \( k' = k \), except for geometric perturbations (translation and rotation). In this case, it uses the lemma for indices whose correspondence stems from the perturbation, e.g., \( k' = k + 1 \) for a translation perturbation that moves the pixels by one coordinate.

**Example.** We next exemplify how VHAGaR uses this lemma. Consider the small network and the occlusion perturbation blackening the pixel \((1, 1)\), presented in Figure 3. VHAGaR sets the dependencies in a matrix, called \( \phi_{dep} \). An entry \((m, k)\) corresponds to the dependency constraint of the neuron \( z_{m,k} \), where the entries at \((0, k)\) store the dependencies of the input neurons and index pairs \((i, j) \in [d_1] \times [d_2] \) are transformed to a single index: \( k = (i - 1) \cdot d_2 + j \in [d_1 \cdot d_2] \). Because the occlusion perturbation is not geometric, we focus on the dependencies of respective neurons \( k = k' \). In this case, the weights are equal and Lemma 5.4 is simplified to the following cases:

- if \( \forall i \ w_{m,k,i} \cdot (z_{m-1,k_i} - z_{m-1,k'_i}^p) = 0 \), then \( z_{m,k} = z_{m',k'}^p \) and \( a_{m,k} = a_{m',k'}^p \).
- if \( \forall i \ w_{m,k,i} \cdot (z_{m-1,k_i} - z_{m-1,k'_i}^p) \geq 0 \), then \( z_{m,k} \geq z_{m',k'}^p \) and \( a_{m,k} \geq a_{m',k'}^p \).
- if \( \forall i \ w_{m,k,i} \cdot (z_{m-1,k_i} - z_{m-1,k'_i}^p) \leq 0 \), then \( z_{m,k} \leq z_{m',k'}^p \) and \( a_{m,k} \leq a_{m',k'}^p \).

VHAGaR first encodes the dependencies stemming from the occlusion perturbation (defined in Section 5.2.2):

- \( \phi_{dep}[0, 1] = \{ z_{0,1} \geq z_{0,1}^p \} \), since \( z_{0,1}^p = 0 \) and \( z_{0,1} \in [0, 1] \), and
- \( \forall k \in [2, 16] \cdot \phi_{dep}[0, k] = \{ z_{0,k} = z_{0,k}^p \} \), since the perturbation does not change these pixels.

These dependencies are propagated to the first layer as follows:

- \( \phi_{dep}[1, 1] = \{ z_{1,1} \geq z_{1,1}^p \} \): This constraint, corresponding to the neuron \( z_{1,1} \), follows from the second bullet of Lemma 5.4. To check that the second bullet holds, VHAGaR inspects
Algorithm 1: Compute_dependency_constraints($f_p$, $I_E$, $\phi_{in}$, $D$, $D^p$)

**Input:** A perturbation $f_p$, its range $I_E$, an input constraint $\phi_{in}$, the network encodings $D$, $D^p$.

**Output:** A dependency constraint $\phi_{dep}$.

1. $\phi_{dep}[L + 1, \max\{k_1, \ldots, k_L\}] = \perp$
2. $\phi_{dep} = \text{add_perturbation}_\text{dep}(f_p, I_E, \phi_{in}, D, D^p, \phi_{dep})$
3. **foreach** layer $m \in \{1, \ldots, L\}$ **do**
   4. **foreach** neuron $k \in \{1, \ldots, k_m\}$ **do**
      5. **if** $\phi_{dep}[m, k] \neq \perp$ **then** continue
      6. **if** $l_{m,k} > u_{m,k}^p$ **then** $\phi_{dep}[m, k] = \{z_{m,k} > z_{m,k}^p, a_{m,k} \geq a_{m,k}^p\}$
      7. **else if** $u_{m,k} < l_{m,k}^p$ **then** $\phi_{dep}[m, k] = \{z_{m,k} < z_{m,k}^p, a_{m,k} \leq a_{m,k}^p\}$
      8. **else**
         9. $\phi_{dep}[m, k] = \text{dependencies}_\text{propagation}(m, k, D, D^p, \phi_{dep})$
         10. **if** $\phi_{dep}[m, k] \neq \perp$ **then** continue
         11. $u_{m,k,m,k} = \infty$, $l_{m,k,m,k} = -\infty$
         12. **if** $[l_{m,k,m,k}, u_{m,k,m,k}] \cap [l_{m,k}^p, u_{m,k}^p] = \emptyset$ **then**
             13. $l_{m,k,m,k} = \min_{\text{cutoff}=0} z_{m,k} - z_{m,k}^p$, s.t. $E' \in I_E, \phi_{in}, \phi_{dep}, D_{1:m}, D_{1:m}^p$
             14. **if** $[l_{m,k,m,k}, u_{m,k,m,k}] \cap [l_{m,k}^p, u_{m,k}^p] = \emptyset$ **then**
                 15. $u_{m,k,m,k} = \max_{\text{cutoff}=0} z_{m,k} - z_{m,k}^p$, s.t. $E' \in I_E, \phi_{in}, \phi_{dep}, D_{1:m}, D_{1:m}^p$
         16. **if** $[l_{m,k,m,k}, u_{m,k,m,k}] = [0, 0]$ **then** $\phi_{dep}[m, k] = \{z_{m,k} = z_{m,k}^p, a_{m,k} = a_{m,k}^p\}$
         17. **else if** $l_{m,k,m,k} \geq 0$ **then** $\phi_{dep}[m, k] = \{z_{m,k} \geq z_{m,k}^p, a_{m,k} \geq a_{m,k}^p\}$
         18. **else if** $u_{m,k,m,k} \leq 0$ **then** $\phi_{dep}[m, k] = \{z_{m,k} \leq z_{m,k}^p, a_{m,k} \leq a_{m,k}^p\}$
      19. **if** $\forall k. \phi_{dep}[m, k] = \perp, m > 0$ **then** break
20. **return** $\bigwedge \{\phi_{dep}[m, k] \mid \phi_{dep}[m, k] \neq \perp, m > 0\}$

the dependency constraints of the inputs to $z_{1,1}$ which are $z_{0,1}, z_{0,2}, z_{0,5}, z_{0,6}$. It observes that $\phi_{dep}[0, 1] = \{z_{0,1} \geq z_{0,1}^p\}$ and $w_{1,1,1} = 1 \geq 0$, and $\phi_{dep}[0, k] = \{z_{1,k} = z_{1,k}^p\}$, for $k \in \{2, 5, 6\}$.

- $\phi_{dep}[1, k] = \{z_{1,k} = z_{1,k}^p\}$, for $k \in \{2, 3, 4\}$: These constraints, corresponding to the neurons $z_{1,2}, z_{1,3},$ and $z_{1,4}$, follow from the first bullet of Lemma 5.4. To check that the first bullet holds, VHAGaR inspects the dependency constraints of the inputs to each of these neurons and observes that all have an equality constraint.

These dependencies are propagated to the second layer as follows:

- $\phi_{dep}[2, 1] = \{z_{2,1} \geq z_{2,1}^p\}$: This constraint, corresponding to the neuron $z_{2,1}$, follows from the second bullet of Lemma 5.4, since $\phi_{dep}[1, 1] = \{z_{1,1} \geq z_{1,1}^p\}$, the weight $w_{2,1,1} = 1$ is positive, and the dependencies of the other inputs are $\phi_{dep}[1, k] = \{z_{1,k} = z_{1,k}^p\}$, for $k > 1$.

- $\phi_{dep}[2, 2] = \{z_{2,2} \leq z_{2,2}^p\}$: This constraint, corresponding to the neuron $z_{2,2}$, follows from the third bullet of Lemma 5.4, since $\phi_{dep}[1, 1] = \{z_{1,1} \geq z_{1,1}^p\}$, the weight $w_{2,1,2} = -1$ is negative, and the dependencies of the other inputs are equalities.

Algorithm. Algorithm 1 shows how VHAGaR computes the dependencies. Its inputs are the perturbation function $f_p$, its range $I_E$, the input constraint $\phi_{in}$, and the classifier encodings $D$ and $D^p$. It begins by initializing a matrix, whose rows are the layers (including the input layer) and its columns are the neurons in each layer. An entry $(m, k)$ in the matrix corresponds to the
dependency constraint of the neuron \( z_{m,k} \) (Line 1). VHAGaR begins by adding the perturbation’s dependencies for the relevant neurons, as defined in Section 5.2.2 (Line 2). VHAGaR then iterates over all neurons, layer-by-layer, and computes dependencies (Line 3–Line 4). If an entry \((m, k)\) is not empty, it is skipped (Line 5). Otherwise, if \( l_{m,k} \), the lower bound of \( z_{m,k} \), is greater than \( u_{m,k}^p \), the upper bound of \( z_{m,k}^p \), then \( z_{m,k} > z_{m,k}^p \) and vice-versa (Line 6–Line 7). Then, VHAGaR checks whether it can propagate a dependency (Line 9). If not, VHAGaR checks whether it should solve the optimization problems. The minimization problem is solved if \( l_{m,k}^p \leq l_{m,k} \leq u_{m,k}^p \leq u_{m,k} \), in which case it may be that \( z_{m,k} \geq z_{m,k}^p \) (Line 12–Line 13). Similarly, the maximization problem is solved if \( l_{m,k} \leq l_{m,k}^p \leq u_{m,k} \leq u_{m,k}^p \), in which case it may be that \( z_{m,k} \leq z_{m,k}^p \) (Line 14–Line 15). Note that if \( l_{m,k}^p < l_{m,k} \leq u_{m,k} < u_{m,k}^p \), the two variables are incomparable since \( z_{m,k}^p \) may be equal to \( l_{m,k}^p \) (and thus \( z_{m,k}^p < z_{m,k} \)) or equal to \( u_{m,k}^p \) (and thus \( z_{m,k} < z_{m,k}^p \)). Similarly, the two variables are incomparable if \( l_{m,k} < l_{m,k}^p \leq u_{m,k}^p < u_{m,k} \). The minimization problem early stops when the lower bound on the objective reaches zero or when the upper bound becomes negative. The maximization problem early stops when the upper bound on the objective reaches zero or when the lower bound becomes positive. This is because the optimal minimal and maximal values are not required to determine the relation between the variables. If both returned values are zero, the variables are equal. Otherwise, if the minimal value is at least zero, then \( z_{m,k} \geq z_{m,k}^p \), and if the maximal value is at most zero, then \( z_{m,k} \leq z_{m,k}^p \) (Line 16–Line 18). The boolean variables have a similar dependency. In case no pair of neurons depends in the current layer, the algorithm terminates (Line 19). Finally, the algorithm returns all dependencies (Line 20).

5.2.2 Input Dependencies. Lastly, we describe the input dependencies \( \phi_{in} \) and define the dependencies’ encoding for several semantic perturbations and the \( L_\infty \) perturbation. These perturbations have been shown to inflict adversarial examples [Engstrom et al. 2019, 2017; Goodfellow et al. 2015; Goswami et al. 2018; Liu et al. 2019b; Mohapatra et al. 2020; Wu et al. 2019]. Our definitions assume grayscale images, for simplicity’s sake, but VHAGaR supports colored images. To align with our MIP encoding, we transform index pairs \((i, j) \in [d_1] \times [d_2]\) to a single index: \( k = (i-1) \cdot d_2 + j \in [d_1 \cdot d_2]\).

**Brightness**([\(l, u\)]). Brightness, whose range is an interval \(-1 \leq l \leq u \leq 1\), adds a value \( E' \in [l, u] \) to the input \( x \). Namely, \( \phi_{in} = \forall k. (z_{0,k}^p = z_{0,k} + E') \). To enhance the encoding’s accuracy, VHAGaR adds constraints depending on the sign of \( E' \). Given the sign of \( E' \), VHAGaR computes the sign of every neuron \( z_{1,k} \) in the first layer, \( s_k = \text{sign}(E' \sum_{i,k,r} w_{i,k,r}) \), and adds \( \phi_{dep}[1, k] = \{z_{1,k} \Rightarrow z_{1,k}^p, a_{1,k} \Rightarrow a_{1,k}^p\} \), where \( \Rightarrow \) if \( s_k = + \), \( \Rightarrow \) if \( s_k = - \), and \( \Rightarrow \) if \( s_k = 0 \). The sign of \( E' \) is \(-\) if \( u \leq 0 \) and \(+\) if \( l \geq 0 \). Otherwise, VHAGaR duplicates the encoding and handles the cases \( E' \in [l, 0] \) and \( E' \in [0, u] \) separately. VHAGaR propagates dependencies to subsequent layers as defined in Lemma 5.4.

**Contrast**([\(l, u\)]). Contrast, whose range is an interval \( l \leq u \in \mathbb{R}^+ \), multiplies the input by a value \( E' \in [l, u] \). Namely, \( \phi_{in} = \forall k. (z_{0,k}^p = E' \cdot z_{0,k}) \). To propagate dependencies as in Lemma 5.4, VHAGaR also adds the constraints \( \forall k. \phi_{dep}[0, k] = \{z_{0,k} \Rightarrow z_{0,k}^p\} \), where \( \Rightarrow \) if \( E' \geq 1 \), and \( \Rightarrow \) if \( E' \leq 1 \). The condition \( E' \geq 1 \) is true if \( l \geq 1 \); the condition \( E' \leq 1 \) is true if \( u \leq 1 \); otherwise, VHAGaR duplicates the encoding and handles the cases \( E' \in [1, l] \) and \( E' \in [1, u] \) separately.

**Occlusion**([\(i_j, i_u\), \([j_l, j_u\), \([w_l, w_u]\)]). Occlusion’s range is three intervals \( 1 \leq i_l \leq i_u \leq d_1 \); \( 1 \leq j_l \leq j_u \leq d_2 \) and \( 1 \leq w_l \leq w_u \leq \min(d_1 - i_u, d_2 - j_u) \). It perturbs based on a triple \((i, j, w)\), where \((i, j)\) defines the top-left coordinate of the occlusion square \( S \) and \( w \) is its length. The perturbation function maps every pixel in the square to zero; other pixels remain as are, namely \( \phi_{in} = \forall k. (k \in \)
\[ S \Rightarrow z_{0,k}^p = 0 \land (k \notin S \Rightarrow z_{0,k}^p = z_{0,k}) \]. To propagate dependencies as in Lemma 5.4, VHAGaR also adds the constraints \( \forall k \in S. \phi_{dep}[0,k] = \{z_{0,k} \geq z_{0,k}^p\} \) and \( \forall k \notin S. \phi_{dep}[0,k] = \{z_{0,k} = z_{0,k}^p\} \).

**Patch**([0, e], [i_j, i_u], [j_j, j_u], [w_l, w_u]). Patch’s range consists of a perturbation limit \( e \in (0, 1) \) and the three intervals as defined in the occlusion’s range. A patch is defined by a top-left coordinate \((i, j)\) of the patch’s square \( S \) and the patch length \( w \). Its function, generalizing occlusion, perturbs every pixel in the patch by a value in the interval \([-e, e]\) (unlike occlusion which maps to zero); other pixels remain as are, namely: \( \phi_{in} = \forall k. (k \in S \Rightarrow z_{0,k}^p \leq z_{0,k} + e \land z_{0,k}^p \geq z_{0,k} - e) \land (k \notin S \Rightarrow z_{0,k}^p = z_{0,k}) \). Note that since every pixel in \( S \) can be perturbed or keep its original value, a patch defined by \((i, j)\) and \( w \) also contains every smaller, subsumed patch. To propagate dependencies as in Lemma 5.4, VHAGaR also adds the constraints \( \forall k \notin S. \phi_{dep}[0,k] = \{z_{0,k} = z_{0,k}^p\} \).

**Translation**([\( t_x^l, t_x^u \)], [\( t_y^l, t_y^u \)]). Translation’s range is two intervals \(-d_1 \leq t_x^l \leq t_x^u \leq d_1 \) and \(-d_2 \leq t_y^l \leq t_y^u \leq d_2 \). Given a coordinate \((t_x, t_y)\), the perturbation function moves every pixel by \((t_x, t_y)\), namely \( \phi_{in} = \forall (i, j). (i - t_x \notin [d_1] \lor j - t_y \notin [d_2] \Rightarrow z_{0,k}^p = 0) \land (i - t_x \in [d_1] \land j - t_y \in [d_2] \Rightarrow z_{0,k}^p = z_{0,k - t_x - d_1 - t_y}) \). To propagate dependencies as in Lemma 5.4, VHAGaR also adds a constraint for every \( k = (i - 1) \cdot d_2 + j: \) if \((i - t_x \notin [d_1] \lor j - t_y \notin [d_2])\) then \( \phi_{dep}[0,k] = \{z_{0,k} \geq z_{0,k}^p\} \), otherwise \( \phi_{dep}[0,k] = \{z_{0,k - t_x - d_1 - t_y} = z_{0,k}^p\} \).

**Rotation**([\( \theta_l, \theta_u \)]). Rotation, whose range is an interval \( 0 \leq \theta_l \leq \theta_u \leq 360 \), rotates the input by an angle \( \theta \) using a standard rotation algorithm. The algorithm\(^1\) begins by centralizing the pixel coordinates, i.e., a coordinate \((i, j)\) is mapped to \( i_c = i - d_1/2 \) and \( j_c = j - d_2/2 \). The centralized coordinates are rotated by \( i_r = i_c \cdot \sin(\theta \cdot \pi/180) + j_c \cdot \cos(\theta \cdot \pi/180) + d_1/2 \) and \( j_r = (i_c \cdot \cos(\theta \cdot \pi/180) - j_c \cdot \sin(\theta \cdot \pi/180) + d_2/2 \). Finally, a bilinear interpolation is executed. Namely, \( \phi_{in} = \forall (i, j). (1 \leq i_r \leq d_1 \land 1 \leq j_r \leq d_2 \Rightarrow z_{0,k}^p = \text{bilinear_interpolation}(z_{0,k, i_r, j_r})) \land (i_r < 1 \lor i_r > d_1 \lor j_r < 1 \lor j_r > d_2 \Rightarrow z_{0,k}^p = 0) \). To propagate dependencies as in Lemma 5.4, VHAGaR also adds a constraint for every \( k = (i - 1) \cdot d_2 + j: \) if \( i_r < 1 \lor i_r > d_1 \lor j_r < 1 \lor j_r > d_2 \), then \( \phi_{dep}[0,k] = \{z_{0,k} \geq z_{0,k}^p\} \). We note that due to the complexity of the rotation algorithm, our implementation currently supports only \( \theta_l = \theta_u \).

\( L_\infty([0, e]) \). \( L_\infty \)'s range is an interval \([0, e]\), where \( e \in (0, 1) \). It perturbs an input by changing every pixel by up to \( |e| \). Namely, \( \phi_{in} = \forall k. z_{0,k}^p \leq z_{0,k} + e \land z_{0,k}^p \geq z_{0,k} - e \).

### 5.3 Suboptimal Lower Bounds and Hints via a Hyper-Adversarial Attack

In this section, we explain how VHAGaR computes suboptimal lower bounds to Problem 2 as well as optimization hints. For every target class \( c_t \in C_t \), VHAGaR computes a lower bound \( \delta_{HA}^{c_t} \geq 0 \), which is added as a constraint to the MIP problem corresponding to \( c_t \), thereby pruning the search space. As part of this computation, VHAGaR identifies hints, guiding the MIP solver towards the maximal globally non-robust bound. The hints are provided to the MIP solver through two matrices \( \mathcal{H}(c_t, c_t) \) for the boolean variables in \( D \), and \( \mathcal{H}(c_t, c_t') \) for \( D' \). An entry \((m, k)\) corresponds to the initial assignment of the boolean variable \( a_{m,k} \) or \( a_{m,k}' \), and it is 0, 1, or \( \perp \) (no assignment).

The lower bound and hints are computed by a hyper-adversarial attack, as described in Section 4.3. Algorithm 2 shows the computation. Its inputs are the classifier \( D \), the class \( c' \), the target class \( c_t \), the perturbation function \( f_p \), its range \( I_c \), and an image dataset \( DS \) (e.g., the dataset used for training the network). VHAGaR begins by expanding the dataset with \(|DS|\) random inputs (uniformly sampled over the range \([0, 1]^{\times d_1 \times d_2}\)), running each input through the classifier, and sorting them by their class confidence of \( c' \) (Line 1). The image dataset is expanded in order to reach inputs

\(^1\)The pseudo-code is provided in the extended version of this paper [Kabaha and Drachsler-Cohen 2024, Appendix A].
Algorithm 2: Compute_lower_bound_and_hints(D, c', c_t, f_p, I_E, DS)

Input: A classifier D, a class c', a target class c_t, a perturbation f_p, its range I_E, a dataset DS.
Output: A lower bound $\delta_{c',c_t}^{HA}$ and boolean hints $H_{c',c_t}^{E}, H_{c',c_t}^{P} \in \{0, 1, \perp\}^{L \times \max(k_1, \ldots, k_L)}$.

1. $\overline{DS} = \text{sort}(DS \cup \text{random_inputs}(|DS|), \lambda x. C(x, c', D))$
2. $cands = \{x \in \overline{DS} \mid C(x, c', D) > 0\}$
3. $X = |cands| > M \? (cands[1 : \lfloor \frac{|cands|}{M} \rfloor] : |cands|) : \overline{DS}[1 : M]$
4. $\tilde{X} = \{\tilde{x}_1, \ldots, \tilde{x}_M\}; \tilde{E}' = \{\tilde{E}'_1, \ldots, \tilde{E}'_M\}$
5. $\text{optimize}(\max_{\tilde{x}_i, \tilde{E}'_i \in I_E} \sum_{i=1}^{M} C(x_i + \tilde{x}_i, c', D) + \sum_{i=1}^{M} \lambda_0 \cdot \frac{||\nabla_{\tilde{x}_i} C(x_i + \tilde{x}_i, c', D)||}{||\nabla_{\tilde{x}_i} \min(C(f_p(x_i + \tilde{x}_i, \tilde{E}'_i), c_t, D), \tau)||} \cdot \min(C(f_p(x_i + \tilde{x}_i, \tilde{E}'_i), c_t, D), \tau))$
6. $\text{sol} = \{x_i \mid C(x_i + \tilde{x}_i, c', D) > 0 \text{ and } C(f_p(x_i + \tilde{x}_i, \tilde{E}'_i), c_t, D) > 0\}$
7. if $\text{sol} \neq \emptyset$ then
8. $\delta_{c',c_t}^{HA} = \max(\{C(x_i + \tilde{x}_i, c', D) \mid x_i \in \text{sol}\})$
9. for $m \in [L]$ and $k \in [k_m]$ do
10. $active = \frac{|D(x_i + \tilde{x}_i)_{m,k} \geq 0 \mid x_i \in \text{sol}|}{|\text{sol}|}; active_p = \frac{|D(f_p(x_i + \tilde{x}_i, \tilde{E}'_i))_{m,k} \geq 0 \mid x_i \in \text{sol}|}{|\text{sol}|}$
11. $H_{c',c_t}^{E}[m,k] = active > r? 1 : (1 - active > r? 0 : \perp)$
12. $H_{c',c_t}^{P}[m,k] = active_p > r? 1 : (1 - active_p > r? 0 : \perp)$
13. return $\delta_{c',c_t}^{HA}, H_{c',c_t}^{E}, H_{c',c_t}^{P}$
14. else return $0, \perp^{L \times \max(k_1, \ldots, k_L)}, \perp^{L \times \max(k_1, \ldots, k_L)}$

with a broader range of class confidences. Accordingly, it identifies candidates for the hyper-input, cands, which are inputs classified as c' (Line 2). Then, it constructs the hyper-input X, consisting of M inputs (where M is a hyper-parameter), as follows. If $|cands|$ is greater than M, VHAGaR selects inputs from candidates in uniform steps of $\lfloor \frac{|cands|}{M} \rfloor$, from one to $|cands|$ (recall that cands is sorted by the class confidence). Otherwise, X is the top-M inputs from the expanded (sorted) dataset (Line 3). It then initializes the input variables $\tilde{X}$ and the perturbation variables $\tilde{E}'$ of Problem 3. VHAGaR adapts the loss described in Section 4.3 to target $c_t$ and maximizes it (Line 5). At high-level, the optimization begins from the hyper-input X (since $\tilde{X}$ is initially the zero vector), consisting of inputs that are classified as c'. The optimization has two objectives: (1) finding inputs $X + \tilde{X}$ and perturbations $\tilde{E}'$ leading to adversarial examples (i.e., the perturbed examples are classified as c_t) and (2) maximizing the class confidence of the inputs in $X + \tilde{X}$. These conflicting goals are balanced using an adaptive term, described shortly. To keep the conflict between the goals to the minimum required, the loss only requires that the perturbed examples’ confidence in c_t is positive. We note that VHAGaR optimizes the loss simultaneously over $\tilde{X}$ and $\tilde{E}'$. The loss optimization is executed by projected gradient descent (PGD) [Madry et al. 2018]. We next describe VHAGaR’s adaptive balancing term $\lambda_t$. This term favors maximization of the class confidence of c_t, as long as $f_p(x_i + \tilde{x}_i, \tilde{E}'_i)$ is not classified as c_t (i.e., the confidence is not positive). Technically, this term multiplies a hyper-parameter $\lambda_0 > 1$ by the ratio of the gradient of the input’s class confidence and the gradient of the perturbed example’s class confidence. Multiplying by this ratio enforces a proportion of $\lambda_0$ between the two optimization goals. Namely, the smaller the second goal’s gradient, the smaller this ratio’s denominator, the larger the ratio and thus the larger the balancing term (which is multiplied by the second goal to keep the $\lambda_0$ proportion between the goals). Similarly, the smaller the first goal’s gradient, the smaller this ratio’s numerator, the smaller
the ratio and thus the smaller the balancing term. To stop maximizing the class confidence \( c_t \) when it is positive, the denominator is the minimum of this confidence and a small number \( \tau > 0 \). To avoid dividing by a number close to zero, the denominator is added a stability hyper-parameter \( \kappa \). After the optimization, VHAGaR retrieves the solutions, i.e., the inputs \( x_i + \hat{x}_i \) classified as \( c' \) whose corresponding perturbed example is classified as \( c_t \) (Line 6). The lower bound \( \delta_{c',c_t}^{\text{HA}} \) is the maximal class confidence of the solutions (Line 8). To compute the hints, the solutions and their perturbed examples are run through the classifier. The hint \( \mathcal{H}_{m,k} \) (or \( \mathcal{H}_{m,k}^p \)) of neuron \((m,k)\) is computed by first counting the number of solutions (or the perturbed examples) in which the neuron is active and dividing by the number of solutions (Line 10). If this ratio is greater than a hyper-parameter \( r \), the hint is 1, if the complementary ratio is greater than \( r \), the hint is 0, otherwise it is \( \perp \) (Line 11-Line 12). VHAGaR returns the lower bound and the hints (Line 13). If no solution is found, the trivial lower bound and empty hints are returned (Line 14).

### 5.4 Correctness and Running Time

In this section, we discuss correctness and running time analysis.

**Correctness.** Our main theorem is that VHAGaR is sound: it returns an interval containing the minimal globally robust bound. Given enough time, the bound is exact up to the precision level \( \Delta \).

**Theorem 5.5.** Given a classifier \( D \), a class \( c' \), a target class set \( C_t \), and a perturbation \((f_p, I_E)\), VHAGaR returns an interval containing the minimal globally robust bound \( \delta_{c',C_t} \in [\delta_{c',C_t}^{\text{L}}, \delta_{c',C_t}^{\text{U}}] \).

**Proof Sketch.** We show that (1) \( \delta_{c',C_t}^{\text{U}} \) is an upper bound on \( \delta_{c',C_t} \) and (2) \( \delta_{c',C_t}^{\text{L}} \) is a lower bound, i.e., there is an input whose class confidence of \( c' \) is \( \delta_{c',C_t}^{\text{L}} - \Delta \) and it has an adversarial example classified as a class in \( C_t \) obtained using the perturbation \((f_p, I_E)\). We prove correctness for every \( c_t \in C_t \) separately and because VHAGaR returns the interval which is the maximum of the bounds, the claim follows. Let \( c_t \in C_t \) and consider its MIP. The MIP soundly encodes Problem 2:

- The encoding of the network’s computation is sound as proven by Tjeng et al. [2019].
- The dependency constraints are sound: if a relation is added between two variables, it must exist. This follows from the perturbations’ definitions and our lemmas. We note that due to scalability, VHAGaR may not identify all dependency constraints, but it only affects the execution time of the anytime algorithm, not the MIP’s soundness.
- The lower bound’s constraint is sound since it is equal to the class confidence of an input satisfying Problem 2’s constraint.

By the anytime operation of the MIP solver, \( \delta_{c',c_t}^{\text{L}} - \Delta \) is a solution to Problem 4’s constraints and thus \( \delta_{c',c_t}^{\text{L}} \) is a lower bound. Similarly, \( \delta_{c',c_t}^{\text{U}} - \Delta \) is an upper bound for the maximally globally non-robust bound, and thus \( \delta_{c',c_t}^{\text{U}} \) is an upper bound for the minimally globally robust bound. \( \square \)

**Running time.** The runtime of VHAGaR is the sum of \( T_e + T_{\text{dep}} + |C_t| \cdot T_{\text{HA}} + |C_t| \cdot T_{\text{MIP}} \), where \( T_e \) is the execution time of the MIP encoding (including the computation of the concrete bounds), \( T_{\text{dep}} \) is the execution time of computing the dependencies \( \phi_{\text{dep}} \), \( T_{\text{HA}} \) is the execution time of the hyper-adversarial attack, and \( T_{\text{MIP}} \) is the MIP solver’s timeout for solving Problem 4. Our implementation reduces the running time by parallelizing the dependency computation on CPUs and parallelizing the hyper-adversarial attack on GPUs. The dominating factor is the execution time of the MIP solver given Problem 4. To mitigate it, our implementation parallelizes the \( |C_t| \) MIPs over CPUs.

### 6 Evaluation

In this section, we evaluate VHAGaR. We begin by describing our experiment setup, then describe the baselines, and then present our experiments.
Table 1. The networks used in our experiments.

| Dataset       | Name | Architecture                                      | #Neurons | Defense |
|---------------|------|---------------------------------------------------|----------|---------|
| MNIST         | 3 × 10 | 3 fully-connected layers                          | 20       | -       |
|               | 3 × 50 | 3 fully-connected layers                          | 100      | PGD     |
|               | conv1  | 2 convolutional (stride 3) and 2 fully-connected layers | 550      | -       |
|               | conv2  | 2 convolutional (stride 1) and 2 fully-connected layers | 2077     | -       |
| Fashion-MNIST | conv1 | 2 convolutional (stride 3) and 2 fully-connected layers | 550      | -       |
| CIFAR-10      | conv1 | 2 convolutional (stride 3) and 2 fully-connected layers | 664      | -       |

Fig. 6. The upper and lower bounds of VHAGaR vs. Marabou and sampling approaches.

Experiment setup. We implemented VHAGaR in Julia, as a module in MIPVerify\(^2\) [Tjeng et al. 2019]. The MIP solver is Gurobi 10.0 and the MIP solver timeout is three hours. The hyperparameters of Algorithm 2 are \(M = 10,000\), \(r = 0.95\) and \(\lambda_0 = 1.01\). Experiments ran on an Ubuntu 20.04.1 OS on a dual AMD EPYC 7713 server with 2TB RAM and eight A100 GPUs. We evaluated VHAGaR over several datasets. MNIST [Lecun et al. 1998] and Fashion-MNIST [Xiao et al. 2017] consist of 28 × 28 grayscale images, showing handwritten digits and clothing items, respectively. CIFAR-10 [Krizhevsky 2009] consists of 3 × 32 × 32 colored images, showing objects (e.g., a dog). We consider several fully-connected and convolutional networks (Table 1). Their activation function is ReLU. The 3 × 50 network is trained with PGD [Madry et al. 2018], an adversarial training defense. While the network sizes may look small compared to the network sizes analyzed by existing local robustness verifiers, we remind that VHAGaR reasons about global robustness. We consider network sizes that are an order of magnitude larger than those analyzed by Reluplex [Katz et al. 2017].

Baselines. We compare VHAGaR to Marabou [Katz et al. 2019], a robustness verifier, and to sampling approaches [Levy et al. 2023; Ruan et al. 2019]. Marabou, which improves Reluplex [Katz et al. 2017], is an SMT-based verifier that can determine whether a classifier is \(\delta\)-globally robust, for a given \(\delta\) (although the paper focuses on local robustness tasks). We extend it to compute the minimal bound using binary search. Since Marabou does not support quadratic constraints, we cannot evaluate it for the contrast perturbation. Recently, a MIP-based verifier for analyzing global robustness of networks (not classifiers) has been proposed [Wang et al. 2022a,b]. However, to date, their code is not available. There are many sampling approaches that estimate a global robustness bound by analyzing the local robustness of input samples [Bastani et al. 2016; Levy et al. 2023; Mangal et al. 2019; Ruan et al. 2019]. These approaches either sample from a given dataset, e.g., DeepTRE [Ruan et al. 2019] that focuses on \(L_0\) perturbations, or from the input domain, e.g., Groma [Levy et al. 2023]. Groma computes the probability of an adversarial example with respect to

\(^2\)https://github.com/vtjeng/MIPVerify.jl
\((\epsilon, \delta)\) global robustness in an input region \(I\), i.e., the probability that inputs in \(I\) whose distance is at most \(\epsilon\) have outputs whose distance is at most \(\delta\): \(\forall x, x' \in I. \|x - x'\| \leq \epsilon \Rightarrow \|D(x) - D(x')\| \leq \delta\). Since these works focus on different global robustness definitions, we adapt their underlying ideas to our setting of computing the minimal global robustness bound, as we next describe. Dataset Sampling computes the maximal globally non-robust bound by first estimating the maximal locally non-robust bound for every input in a given dataset of 70,000 samples. Then, it returns the maximal bound, denoted \(\delta^{DS}\). Random Sampling draws 70,000 independent and identically distributed (i.i.d.) samples from the input domain, assuming a Gaussian distribution, and computes the maximal locally non-robust bound for each. Like Groma, it returns the average non-robust bound \(\delta^{RS}\) along with its 5% confidence interval \(h\) derived using the Hoeffding inequality [Hoeffding 1963], i.e., \(\delta^{RS} \pm h\). Due to the very high number of samples, the baseline implementations cannot compute the maximal locally non-robust bound by running the MIP verifier. Instead, we take the following approach. For discrete perturbations (e.g., occlusion), we perturb the input samples and run them through the classifier. We filter the perturbed examples that are misclassified and accordingly, we infer the maximal locally non-robust bound. For continuous perturbations (e.g., brightness), we run the verifier VeeP [Kabaha and Drachsler-Cohen 2022], designated for semantic perturbations. Note that the sampling baselines provide only a lower bound on the maximal globally non-robust bound. Nevertheless, our baseline implementations are challenging for VHAGaR: the number of samples is 70,000, whereas DeepTRE is evaluated on at most 5,300 samples and Groma on 100 samples.

Comparison to baselines. We begin by evaluating all approaches for various perturbations: \(L_{\infty}\), brightness, contrast, occlusion, patch, translation, and rotation. For each perturbation, we run each approach for every network and every pair of a class \(c'\) and a target class (i.e., \(C_t = \{c_t\}\)) for three hours. We record the execution time in minutes \(t\), the lower bound \(\delta^{L}_{c'}\) (of all approaches) and the upper bound \(\delta^{U}_{c'}\) (of VHAGaR and Marabou). To put these values in context, we report the average percentages, \(\delta^{L}_{c'}/\delta^{U}_{c'} \cdot 100\) and \(\delta^{U}_{c'}/\delta^{U}_{c'} \cdot 100\), where \(\delta^{U}_{c'}\) is the maximum class confidence of \(c'\) (computed by solving \(\max C(x, c', D) \text{ subject to } D\)). We further report \(\delta^{d}_{c'} \equiv \delta^{U}_{c'}/\delta^{U}_{c'} \cdot 100\), where \(\delta^{d}_{c'}\) is the maximum class confidence of \(c'\) in the given dataset. Although global robustness provides guarantees for any input, including those outside the dataset, this value provides context to how close (and relevant) the minimal globally robust bound is to dataset-like inputs. Figure 6 provides a visualization of the bounds computed by all approaches, while Table 2 provides the results for MNIST and Table 3 for Fashion-MNIST and CIFAR-10 (we provide more results in the extended version of this paper [Kabaha and Drachsler-Cohen 2024, Appendix B]). We note that if a perturbation’s parameter (e.g., a coordinate or an angle) is over \([v, v]\), we denote it as \(v\). Our results show that (1) the average gap between the lower and upper bound of VHAGaR is 1.9, while Marabou’s gap is 154.7, (2) the lower bound of VHAGaR is greater (i.e., tighter) by 6.5x than Marabou, by 9.2x than Dataset Sampling and by 25.6x than Random Sampling, (3) the upper bound of VHAGaR is smaller (i.e., tighter) by 13.1x than Marabou, (4) VHAGaR is 130.6x faster than Marabou, and (5) VHAGaR is 115.9x slower than the sampling approaches (which is not surprising since these provide very loose lower bounds). The ratio of VHAGaR’s \(\delta^{U}\) and \(\delta^{U}\) is 0.37 on average. Additionally, although VHAGaR considers any possible input, the results show that our \(\delta\)-globally robust bounds overlap with the class confidences of the dataset’s inputs: the ratio of VHAGaR’s \(\delta^{U}\) and \(\delta^{d}\) is between 0.02-3.1 and on average 0.9. We note that without the rotation and translation, whose bounds are relatively high, the average is 0.69. The high bounds of small rotation and translation perturbations are counterintuitive, since these perturbations are imperceptible. These counterintuitive bounds highlight the importance of computing the minimum globally robust bounds to gain insights into the network’s global robustness against perturbations. In the extended version of this paper, we show the execution time of each component of VHAGaR.
Table 2. VHAGaR vs. Marabou and sampling approaches over MNIST classifiers. Sa. abbreviates Sampling, Occ. Occlusion, Pa. Patch, and Trans. Translation. NA is not applicable.

| Model | Perturbation | VHAGaR | Marabou | Dataset Sa. | Random Sa. |
|-------|--------------|--------|---------|-------------|------------|
|       |              | $\delta^l$ | $\delta^u$ | $t$ | $\delta^l$ | $\delta^u$ | $t$ | $\delta^{DS}$ | $t$ | $\delta^{RS \pm h}$ | $t$ |
|       |              | % | % | [m] | % | % | [m] | % | [m] | % | [m] |
| MNIST | Brightness([0,0.25]) | 28 | 28 | 0.1 | 24 | 114 | 161 | 8 | 0.5 | 1.7 ± 0.2 | 0.2 |
| 3 × 10 | Brightness([0,0.1]) | 15 | 15 | 0.2 | 12 | 115 | 158 | 6 | 0.4 | 1.5 ± 0.2 | 0.2 |
| $\delta^m = 45$ | Contrast([1,1.5]) | 0.4 | 0.8 | 180 | NA | NA | NA | 0.2 | 0.5 | 0.08 ± 0.5 | 0.7 |
| $\delta^d = 29\%$ | Contrast([1,2]) | 0.6 | 1.6 | 180 | NA | NA | NA | 0.4 | 0.4 | 0.1 ± 0.4 | 0.7 |
| MNIST | $L_\infty([0,0.1])$ | 80 | 81 | 0.4 | 54 | 154 | 180 | 42 | 0.1 | 6.1 ± 10.6 | 0.2 |
| 3 × 50 | $L_\infty([0,0.05])$ | 57 | 58 | 3.9 | 30 | 157 | 180 | 26 | 0.2 | 7.0 ± 7.8 | 0.3 |
| $\delta^m = 3.3$ | Brightness([0,0.25]) | 37 | 38 | 5.3 | 27 | 153 | 136 | 11 | 0.4 | 4 ± 0.6 | 0.4 |
| $\delta^d = 73\%$ | Brightness([0,0.1]) | 22 | 22 | 9.2 | 15 | 154 | 141 | 6 | 0.4 | 3 ± 0.6 | 0.4 |
| Occ.(14,14,9) | 72 | 72 | 0.8 | 64 | 152 | 133 | 19 | 0.2 | 3 ± 0.7 | 0.5 |
| Occ.(1,1,9) | 4 | 4 | 99 | 3 | 148 | 149 | 0.03 | 0.3 | 0.2 ± 0.1 | 0.2 |
| Pa.([0,1],14,14,[1,9]) | 93 | 93 | 0.2 | 90 | 149 | 115 | 33 | 0.3 | 3 ± 0.4 | 0.5 |
| Pa.([0,1],1,1,[1,9]) | 7 | 8 | 71 | 4 | 154 | 140 | 0.6 | 0.4 | 4 ± 0.5 | 0.5 |
| Pa.([0,1],1,1,[1,5]) | 2 | 3 | 131 | 2 | 151 | 150 | 0.1 | 0.2 | 0.3 ± 0.1 | 0.5 |
| MNIST | $L_\infty([0,0.1])$ | 49 | 59 | 166 | 17 | 202 | 180 | 18 | 0.2 | 4.3 ± 0.6 | 0.2 |
| $\delta^m = 38.6$ | Brightness([0,0.25]) | 29 | 31 | 94 | 4 | 300 | 180 | 6 | 0.6 | 1.1 ± 0.7 | 0.2 |
| $\delta^d = 30\%$ | Brightness([0,0.1]) | 15 | 22 | 180 | 5 | 233 | 180 | 4 | 0.8 | 1.0 ± 0.5 | 0.3 |
| Occ.(14,14,9) | 36 | 36 | 1.2 | 10 | 195 | 180 | 9 | 0.5 | 1.7 ± 0.5 | 0.3 |
| Occ.([12,16],[12,16],5) | 37 | 37 | 3.3 | 21 | 307 | 180 | 10 | 2.5 | 1.2 ± 2.4 | 0.9 |
| Occ.(1,1,5) | 9 | 9 | 0.8 | 4 | 300 | 180 | 0 | 0.5 | 0.4 ± 1.8 | 0.2 |
| Pa.([0,1],14,14,[1,9]) | 36 | 36 | 0.5 | 17 | 191 | 180 | 13 | 0.4 | 3.6 ± 0.5 | 0.5 |
| Pa.([0,1],14,14,[1,5]) | 26 | 27 | 0.4 | 5 | 212 | 180 | 12 | 0.4 | 2.1 ± 0.7 | 0.5 |
| Pa.([0,1],14,14,[1,3]) | 13 | 13 | 0.8 | 24 | 187 | 180 | 6 | 0.4 | 1.3 ± 1.0 | 0.5 |
| Trans.([1,3],[1,3]) | 93 | 93 | 3.8 | 5 | 212 | 180 | 19 | 1.6 | 2.5 ± 1.5 | 1.1 |
| Rotation(10°) | 80 | 81 | 1.6 | 33 | 155 | 180 | 11 | 1.5 | 1.5 ± 0.6 | 1.3 |
| MNIST | $L_\infty([0,0.05])$ | 25 | 25 | 4.9 | 0 | 304 | 180 | 12 | 0.3 | 1.4 ± 0.02 | 0.3 |
| $\delta^m = 147$ | Brightness([0,0.1]) | 9 | 13 | 88 | 7 | 328 | 180 | 5 | 0.7 | 0.4 ± 0.03 | 0.3 |
| $\delta^d = 42\%$ | Occ.([12,16],[12,16],5) | 37 | 38 | 4.9 | 0 | 321 | 180 | 17 | 3.3 | 2.5 ± 1.0 | 1.6 |
| Occ.(14,14,5) | 18 | 19 | 2.4 | 0 | 307 | 180 | 5 | 1.3 | 0.3 ± 0.07 | 0.2 |
| Occ.(1,1,5) | 2 | 2 | 1.8 | 1 | 303 | 180 | 0 | 1.2 | 0.1 ± 0.1 | 0.3 |
| Pa.([0,1],14,14,[1,9]) | 61 | 61 | 0.7 | 1 | 303 | 180 | 14 | 0.4 | 0.9 ± 0.06 | 0.4 |
| Pa.([0,1],14,14,[1,5]) | 27 | 28 | 2.7 | 0 | 301 | 180 | 11 | 0.5 | 0.8 ± 0.05 | 0.4 |
| Pa.([0,1],14,14,[1,3]) | 13 | 13 | 3 | 0 | 295 | 180 | 7 | 0.4 | 0.6 ± 0.07 | 0.4 |
| Trans.([1,3],[1,3]) | 99 | 99 | 3.5 | 0 | 302 | 180 | 25 | 1.1 | 1.0 ± 0.02 | 1.2 |
| Rotation(10°) | 79 | 79 | 1.6 | 0 | 595 | 180 | 15 | 1.0 | 0.7 ± 0.05 | 0.9 |

Interpreting robustness bounds. We next show how our minimal globally robust bounds can reveal global robustness and vulnerability attributes of a network to an adversarial attack. We consider two experiments, both focus on the conv1 network for MNIST. In the first experiment, we compare the network’s global robustness bounds for four perturbations. For each perturbation, VHAGaR
Table 3. VHAGaR vs. Marabou and sampling approaches over Fashion-MNIST and CIFAR-10 classifiers. Sa. abbreviates Sampling, Occ. Occlusion, Pa. Patch, and Trans. Translation.

| Model       | Perturbation | VHAGaR | Marabou | Dataset Sa. | Random Sa. |
|-------------|--------------|--------|---------|-------------|------------|
|             |              | $\delta^l$ | $\delta^u$ | $t$ | $\delta^l$ | $\delta^u$ | $t$ | $\delta^l$ | $\delta^u$ | $t$ | $\delta^l$ | $\delta^u$ | $t$ | $\delta^l$ | $\delta^u$ | $t$ |
| FMNIST      | $L_{\infty}([0,0.05])$ | 30 | 59 | 178 | 7 | 224 | 180 | 16 | 0.3 | 6.1 ± 0.5 | 0.5 |
| conv1       | Brightness([0,0.1]) | 17 | 52 | 118 | 14 | 130 | 180 | 2 | 0.4 | 0.5 ± 0.1 | 0.6 |
| $\delta^m$ = 66.8 | Occ.(14,14,9) | 32 | 33 | 19 | 28 | 154 | 180 | 5 | 0.6 | 0.4 ± 0.2 | 0.4 |
| $\delta^d$ = 46% | Occ.(1,1,5) | 17 | 17 | 2.9 | 15 | 151 | 180 | 0.2 | 0.5 | 0.3 ± 0.15 | 0.4 |
|             | Pa.([0,1],14,14,[1,9]) | 52 | 53 | 12.5 | 42 | 151 | 180 | 7 | 0.2 | 0.8 ± 0.1 | 0.5 |
|             | Pa.([0,1],14,14,[1,5]) | 19 | 19 | 5.2 | 15 | 151 | 180 | 4 | 0.2 | 0.7 ± 0.1 | 0.5 |
|             | Pa.([0,1],14,14,[1,3]) | 11 | 11 | 9.2 | 7 | 153 | 180 | 2 | 0.2 | 0.3 ± 0.1 | 0.5 |
|             | Trans.([1,3],[1,3]) | 92 | 94 | 16.3 | 90 | 151 | 180 | 17 | 1.1 | 0.9 ± 0.15 | 0.7 |
|             | Rotation(10°) | 89 | 89 | 2.1 | 82 | 151 | 180 | 11 | 1.6 | 1.05 ± 0.2 | 1.1 |
| CIFAR-10    | Occ.(1,1,9) | 24 | 25 | 7.4 | 1 | 308 | 180 | 6 | 0.3 | 0.6 ± 0.2 | 0.3 |
| conv1       | Occ.(1,1,5) | 15 | 18 | 26 | 0 | 300 | 180 | 3 | 0.3 | 0.4 ± 0.3 | 0.2 |
| $\delta^m$ = 54.3 | Pa.([0,1],14,14,[1,9]) | 51 | 62 | 63 | 1 | 313 | 180 | 8 | 0.5 | 1.3 ± 0.1 | 0.6 |
| $\delta^d$ = 43% | Pa.([0,1],14,14,[1,5]) | 29 | 34 | 35 | 0 | 303 | 180 | 3 | 0.5 | 0.7 ± 0.2 | 0.5 |
|             | Pa.([0,1],14,14,[1,3]) | 14 | 22 | 51 | 0 | 307 | 180 | 1 | 0.5 | 0.4 ± 0.3 | 0.5 |

Fig. 7. Visualization of the minimal globally robust bounds over various perturbations, for MNIST conv1.

computes the minimal globally robust bound for every pair of a class $c'$ and a target class $C_t = \{c_t\}$. Figure 7(a) shows the bounds in a heatmap: the darker the entry the lower the minimal globally robust bound is (i.e., the harder it is to attack). The heatmaps demonstrate the following. First, there is a significant variance in the bounds of different target classes, e.g., $\delta_{c'=5,c_t=0} = 13$ while $\delta_{c'=5,c_t=4} = 2$, for occlusion(14,14,5). Namely, any image whose class confidence in $c' = 5$ is more
than 2 cannot be classified as $c_t = 4$ by this attack. Among these images, only those whose class confidence is more than 13 are globally robust to $c_t = 0$ for this attack. That is, it is significantly harder for an attacker to use this occlusion attack in order to fool conv1 into classifying a five-digit image as a four, compared to as a zero. Second, there is a variance in the effectiveness of the perturbation attacks. As expected, the bounds of the occlusion(14,14,5) attack are smaller or equal to the bounds of the patch([0,1],14,14,[1,5]) attack, which subsumes it. Also, generally, the classifier is less globally robust to the brightness([0,0.5]) attack compared to the occlusion(14,14,5) and the patch([0,1],14,14,[1,5]) attacks. Surprisingly, even though the translation(0,1) attack may seem to introduce a small perturbation, the classifier is not very robust to it (i.e., the bounds are very high). By such inspection, VHAGaR enables the network designer to understand what perturbations are effective or not. The second experiment looks for spatial attributes of global robustness. In this experiment, we partition pixels into $4 \times 4$ squares and the goal is to identify which squares are more robust to arbitrary perturbations, for inputs classified as $c' = 0$. This can be determined by running VHAGaR with the patch perturbation for every square with $\epsilon = 1$. Figure 7(b) shows heatmaps over the input dimension, where each entry shows the bound returned by VHAGaR for $C_t = \{c_t\}$, for $c_t \in \{1, 2, 3\}$, and $C_t = \{1, \ldots, 9\}$ (i.e., an untargeted attack). The heatmaps show that conv1 is more vulnerable to patch attacks at the center, where the digit is located. However, an untargeted attack can also mislead the network by perturbing a patch in the background. These heatmaps allow the network designer to identify vulnerable pixel regions and consider a suitable defense.

**Ablation study.** Lastly, we study the importance of VHAGaR’s components via an ablation study. We compare VHAGaR to two variants: MIP-only and MIP + dependencies (i.e., without the hyper-adversarial attack). We run all approaches over the MNIST $3 \times 50$ and conv1 networks and various perturbation types, for every pair of classes $c'$ and $c_t$, for three hours. We measure the execution time in minutes $t$, the upper bound $\delta^u$, and the lower bound $\delta^l$. We also measure the minute at which the first non-trivial lower bound is obtained $t^l$ and its corresponding class confidence $\delta^l$. Note that the first non-trivial lower bound depends on the variant’s MIP (i.e., with/without dependencies, with/without the hyper-adversarial attack’s lower bound). The higher the $\delta^l$ the closer the optimization is to the maximal globally non-robust bound, while the shorter the $t^l$ the more efficient the variant is. Table 4 shows the results. Our results indicate that VHAGaR is 1.7x faster than the MIP + dependencies variant and 78.6x faster than the MIP-only variant. Given three hours, the upper and lower bounds of VHAGaR are identical to the MIP + dependencies variant. The lower bound of the MIP-only variant is smaller (i.e., looser) by 1.3x and the upper bound is greater (i.e., looser) by 1.05x. Also, the results show that Algorithm 2 allows VHAGaR to obtain

| Perturbation       | VHAGaR | Our MIP + dependencies | Our MIP |
|--------------------|--------|-------------------------|---------|
|                    | $t$    | $t^l$ | $\delta^l$ | $\delta^u$ | $\delta^l$ | $t$    | $t^l$ | $\delta^l$ | $\delta^u$ | $\delta^l$ |
| Brightness([0,1])  | 4.8    | 0.1  | 48  | 48  | 39  | 8.2  | 1.2  | 48  | 48  | 30  | 18.5 | 1  | 48  | 48  | 27  |
| Brightness([0,0.1])| 9.2    | 0.2  | 21  | 21  | 15  | 18   | 1    | 21  | 21  | 12  | 34.6 | 1.1 | 21  | 21  | 15  |
| Brightness([0,1])  | 44     | 0.7  | 44  | 44  | 36  | 53   | 6.5  | 44  | 44  | 31  | 91   | 12  | 44  | 49  | 23  |
| Occ.(14,14,3)      | 0.9    | 0.15 | 9   | 9   | 7   | 0.9  | 0.3  | 9   | 9   | 6   | 180  | 40  | 4   | 24  | 3   |
| Pa.([0,1],14,14,[1,3]) | 0.7 | 0.05 | 13  | 13  | 6   | 0.9  | 0.1  | 13  | 13  | 5   | 180  | 0.1 | 13  | 34  | 4   |
| Translation(3,3)   | 0.1    | 0.01 | 93  | 93  | 90  | 0.3  | 0.1  | 93  | 93  | 67  | 0.5  | 0.1 | 93  | 93  | 66  |
1.2x (1.5x) higher (i.e., tighter) initial solutions in 6.7x (52.6x) less time compared to the MIP + dependencies variant (MIP-only variant).

7 RELATED WORK

Global robustness. Several works analyze global robustness properties. Katz et al. [2017]; Wang et al. [2022a,b] compute the worst-case change in a network’s outputs but cannot determine whether the classification changes. Bastani et al. [2016]; Ruan et al. [2019] estimate global robustness for classifiers from local robustness guarantees. Levy et al. [2023]; Mangal et al. [2019] provide probabilistic guarantees. Gopinath et al. [2018] compute robustness guarantees for input regions derived from the dataset; however, these do not imply global robustness for any input. Leino et al. [2021]; Zhang et al. [2022a] train $L_\infty$ globally robust networks using Lipschitz bounds. Sun et al. [2022] are similar but focus on SDN (and not DNN). Chen et al. [2021] propose booster-fixer training for security classifiers to enforce global robustness.

Multiple network encodings. VHAGaR encodes two copies of a network classifier to analyze the output of an input and its perturbed example. Several works employed a similar concept. Katz et al. [2017]; Wang et al. [2022a,b] analyze global robustness by reasoning about two copies of a network (not a classifier) and bounding the maximum change in the output caused by an input perturbation. Wang et al. [2022a,b] leverage dependencies defined by interleaving connections which capture the difference of respective neurons to overapproximate their functionality. In contrast, VHAGaR integrates dependencies stemming from the perturbation, propagates dependencies across layers, and infers neurons’ dependencies from their concrete bounds or by solving suitable MIP problems. Other works reason about multiple networks in the context of differential analysis [Mohammadinejad et al. 2021] or local robustness of an ensemble of networks [Yang et al. 2022].

Attack-guided verification. VHAGaR executes a hyper-adversarial attack to obtain suboptimal feasible solutions. Several works integrate adversarial attacks with verification. Anderson et al. [2019] improve a verifier’s precision by leveraging spurious adversarial examples. Balunovic and Vechev [2020] employ attacks to train networks with provable robustness guarantees. Dimitrov et al. [2022] leverage the PGD attack [Madry et al. 2018] to compute robust adversarial regions.

8 CONCLUSION

We present VHAGaR for computing the minimal globally robust bound of a neural network classifier, under a given perturbation. VHAGaR encodes the problem as a MIP, over two network copies, which is solved in an anytime manner. VHAGaR reduces the MIP’s complexity by (1) identifying dependencies stemming from the perturbation and the network’s computation and (2) generalizing adversarial attacks to unknown inputs to compute suboptimal lower bounds and optimization hints. Our results show that VHAGaR computes 82.2x tighter bounds and is 130.6x faster than an existing global robustness verifier. Additionally, its lower bounds are 18.8x higher than those computed by sampling approaches. Finally, we exemplify how VHAGaR can provide insights into the robustness attributes of a network classifier to adversarial attacks.

ACKNOWLEDGEMENTS

We thank the anonymous reviewers for their feedback. This research was supported by the Israel Science Foundation (grant No. 2605/20).

DATA-AVAILABILITY STATEMENT

Our code is available at https://github.com/ananmkabaha/VHAGaR.git.
REFERENCES

Motasem Alfarra, Adel Bibi, Hasan Hammoud, Mohamed Gaafar, and Bernard Ghanem. 2020. On the Decision Boundaries of Neural Networks: A Tropical Geometry Perspective. In abs/2002.08838 (2020). https://doi.org/10.1109/TPAMI.2022.3201490

Greg Anderson, Shankara Pailoor, Isil Dillig, and Swarat Chaudhuri. 2019. Optimization and abstraction: a synergistic approach for analyzing neural network robustness. In PLDI (2019). https://doi.org/10.1145/3314221.3314614

Mislav Balunovic, Maximilian Baader, Gagandeep Singh, Timon Gehr, and Martin T. Vechev. 2019. Certifying Geometric Robustness of Neural Networks. In NeurIPS (2019). https://proceedings.neurips.cc/paper/2019/hash/f7fa6aca028e7ff4ef62d75ed025fe76-Abstract.html

Mislav Balunovic and Martin T. Vechev. 2020. Adversarial Training and Provable Defenses: Bridging the Gap. In ICLR (2020). https://openreview.net/forum?id=SJxSDxrKDr

Osbert Bastani, Yani Ioannou, Leonidas Lampropoulos, Dimitrios Vytiniotis, Aditya V. Nori, and Antonio Criminisi. 2016. Measuring Neural Net Robustness with Constraints. In NeurIPS (2016). https://proceedings.neurips.cc/paper/2016/hash/980ecd059122ce2e50136bda65c25e07-Abstract.html

Anand Bhattad, Min Jin Chong, Kaizhao Liang, Bo Li, and David A. Forsyt. 2020. Unrestricted Adversarial Examples via Semantic Manipulation. In ICLR (2020). https://openreview.net/forum?id=Sye_OgHFwH

Nicholas Carlini and David A. Wagner. 2017. Towards Evaluating the Robustness of Neural Networks. In SP (2017). https://doi.org/10.1109/SP.2017.49

Pin-Yu Chen, Yash Sharma, Huan Zhang, Jinfeng Yi, and Cho-Jui Hsieh. 2018. EAD: Elastic-Net Attacks to Deep Neural Networks via Adversarial Examples. In AAAI (2018). https://doi.org/10.1609/aaai.v32i1.11302

Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. 2017. ZOO: Zeroth Order Optimization Based Black-box Attacks to Deep Neural Networks without Training Substitute Models. In AI Sec Workshop (2017). https://doi.org/10.1145/3128572.3140448

Yizheng Chen, Shiqi Wang, Yue Qin, Xiaojing Liao, Suman Jana, and David A. Wagner. 2021. Learning Security Classifiers with Verified Global Robustness Properties. In CCS (2021). https://doi.org/10.1145/3460120.3484776

Dimitar Iliev Dimitrov, Gagandeep Singh, Timon Gehr, and Martin T. Vechev. 2022. Provably Robust Adversarial Examples. In ICLR (2022). https://openreview.net/forum?id=UMfhoMtIaP5

Rüdiger Ehlers. 2017. Formal verification of piece-wise linear feed-forward neural networks. In ATVA (2017). https://doi.org/10.1007/978-3-319-68167-2_19

Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry. 2019. Exploring the Landscape of Spatial Robustness. In ICML (2019). http://arxiv.org/abs/1712.02779

Logan Engstrom, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry. 2017. A Rotation and a Translation Suffice: Fooling CNNs with Simple Transformations. In abs/1712.02779 (2017). https://openreview.net/pdf?id=BJfvknCqFQ

Ecenaz Erdemir, Jeffrey Bickford, Luca Melis, and Sergül Aydöre. 2022. Adversarial Robustness with Non-uniform Perturbations. In NeurIPS (2022). https://openreview.net/pdf?id=cQLkLAQgZ5I

Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and Harnessing Adversarial Examples. In ICLR (2015). http://arxiv.org/abs/1412.6572

Divya Gopinath, Guy Katz, Corina S. Pasareanu, and Clark W. Barrett. 2018. DeepSafe: A Data-Driven Approach for Assessing Robustness of Neural Networks. In ATVA (2018). https://doi.org/10.1007/978-3-030-01090-4_1

Gaurav Goswami, Nalini K. Ratha, Akshay Agarwal, Richa Singh, and Mayank Vatsa. 2018. Unravelling Robustness of Deep Learning based Face Recognition Against Adversarial Attacks. In AAAI (2018). https://doi.org/10.48550/arXiv.1803.00401

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual Learning for Image Recognition. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR. IEEE Computer Society, 770–778. http://arxiv.org/abs/1512.03385

W. Hoeffding. 1963. Probability inequalities for sums of bounded random variables. J. Amer. Statist. Assoc. 58, 301 (1963), 13–30. https://doi.org/10.1007/978-1-4612-0865-5_26

Changcun Huang. 2020. ReLU Networks Are Universal Approximators via Piecewise Linear or Constant Functions. In Neural Computation 32, 11 (11 2020), 2249–2278. https://doi.org/10.1162/neco_a_01316

Anan Kabaha and Dana Drachsler-Cohen. 2022. Boosting Robustness Verification of Semantic Feature Neighborhoods. In SAS (2022). https://doi.org/10.1007/978-3-031-22308-2_14

Anan Kabaha and Dana Drachsler-Cohen. 2023. Maximal Robust Neural Network Specifications via Oracle-Guided Numerical Optimization. In VMCAI (2023). https://doi.org/10.1007/978-3-031-24950-1_10

Anan Kabaha and Dana Drachsler-Cohen. 2024. Verification of Neural Networks’ Global Robustness. In CoRR abs/2402.19322 (2024). https://doi.org/10.48550/arXiv.2402.19322

Guy Katz, Clark W. Barrett, David L. Dill, Kyle Julian, and Mykel J. Kochenderfer. 2017. Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks. In CAV (2017). https://doi.org/10.1007/978-3-319-63387-9_5

Guy Katz, Derek A. Huang, Duligur Ibeling, Kyle Julian, Christopher Lazarus, Rachel Lim, Parth Shah, Shantanu Thakoor, Haoze Wu, Aleksandar Zeljic, David L. Dill, Mykel J. Kochenderfer, and Clark W. Barrett. 2019. The Marabou Framework.
for Verification and Analysis of Deep Neural Networks. In CAV (2019). https://doi.org/10.1007/978-3-030-25540-4_26

Alex Krizhevsky. 2009. Learning Multiple Layers of Features from Tiny Images. (2009). https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf

Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. 1998. Gradient-based Learning Applied to Document Recognition. In Proceedings of the IEEE 1998;86(11):2278e324 (1998). https://doi.org/10.1109/5.726791

Klas Leino, Zifan Wang, and Matt Fredrikson. 2021. Globally-Robust Neural Networks. In ICML (2021). http://proceedings.mlr.press/v139/leino21a.html

Natan Levy, Raz Yerushalmi, and Guy Katz. 2023. gRoMA: a Tool for Measuring Deep Neural Networks Global Robustness. In arXiv.2301.02288 (2023). https://doi.org/10.48550/arXiv.2301.02288

Changjiang Li, Shouling Ji, Haiqin Weng, Bo Li, Jie Shi, Raheem Beyah, Shanqing Guo, Zonghui Wang, and Ting Wang. 2021. Towards Certifying the Asymmetric Robustness for Neural Networks: Quantification and Applications. In TDSC (2021). https://doi.org/10.1109/TDSC.2021.3116105

Chen Liu, Ryota Tomioka, and Volkan Cevher. 2019a. On Certifying Non-Uniform Bounds against Adversarial Attacks. In ICML (2019). http://proceedings.mlr.press/v97/liu19h.html

Xin Liu, Huanrui Yang, Ziwei Liu, Linghao Song, Yiran Chen, and Hai Li. 2019b. DPATCH: An Adversarial Patch Attack on Object Detectors. In AAAI (2019). https://doi.org/10.48550/arXiv.1806.02299

Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. 2018. Towards Deep Learning Models Resistant to Adversarial Attacks. In ICLR (2018). https://doi.org/forum?id=rJzIBfZAb

Ravi Mangal, Aditya V. Nori, and Alessandro Orso. 2019. Robustness of neural networks: a probabilistic and practical approach. In ICSE (2019). https://doi.org/10.1109/ICSE-NIER.2019.00032

Sara Mohammadinejad, Brandon Paulsen, Jyotirmoy V. Deshmukh, and Chao Wang. 2021. DiffRNN: Differential Verification of Recurrent Neural Networks. In FORMATS (2021). https://doi.org/10.1007/978-3-030-85037-1_8

Jeet Mohapatra, Tsui-Wei Weng, Pin-Yu Chen, Sijia Liu, and Luca Daniel. 2020. Towards Verifying Robustness of Neural Networks Against A Family of Semantic Perturbations. In CVPR (2020). https://doi.org/10.48550/arXiv.1912.09533

Chongli Qin, Krishnamurthy (Dj) Dvijotham, Brendan O’Donoghue, Rudy Bunel, Robert Stanforth, Sven Gowal, Jonathan Usato, Grzegorz Swirszcz, and Pushmeet Kohli. 2019. Verification of Non-Linear Specifications for Neural Networks. In ICLR (2019). https://openreview.net/forum?id=HyFAsRctQ

Wenjie Ruan, Min Wu, Youcheng Sun, Xiaowei Huang, Daniel Kroening, and Marta Kwiatkowska. 2019. Global Robustness Evaluation of Deep Neural Networks with Provable Guarantees for the Hamming Distance. In IJCAI (2019). https://doi.org/10.24963/IJCAI.2019/824

Gagandeep Singh, Rupanshu Ganvir, Markus Püschel, and Martin T. Vechev. 2019a. Beyond the Single Neuron Convex Barrier for Neural Network Certification. In NeurIPS (2019). https://proceedings.neurips.cc/paper/2019/hash/0a9fdbb17feb6ccb7ec405cfb85222c4-Abstract.html

Gagandeep Singh, Timon Gehr, Markus Püschel, and Martin T. Vechev. 2019b. An abstract domain for certifying neural networks. In POPL (2019). https://doi.org/10.1145/3290354

Gagandeep Singh, Timon Gehr, Markus Püschel, and Martin T. Vechev. 2019c. Boosting Robustness Certification of Neural Networks. In ICLR (2019). https://openreview.net/forum?id=HJgeEh09KQ

Weidi Sun, Yuteng Lu, Xiyue Zhang, and Meng Sun. 2022. DeepGlobal: A framework for global robustness verification of feedforward neural networks. In J. Syst. Archit. (2022). https://doi.org/10.1016/J.SYSARC.2022.102582

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. 2014. Intriguing Properties of Neural Networks. In ICLR (2014). https://doi.org/10.48550/arXiv.1312.6199

Vincent Tjeng, Kai Y. Xiao, and Russ Tedrake. 2019. Evaluating robustness of neural networks with mixed integer programming. In ICLR (2019). https://openreview.net/forum?id=HyGIdiRqtm

Chun-Chen Tu, Pai-Shun Ting, Pin-Yu Chen, Sijia Liu, Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh, and Shin-Ming Cheng. 2019. AutoZOOM: Autoencoder-Based Zeroth Order Optimization Method for Attacking Black-Box Neural Networks. In AAAI (2019). http://arxiv.org/abs/1805.11770

Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana. 2018. Efficient Formal Safety Analysis of Neural Networks. In NeurIPS (2018). https://proceedings.neurips.cc/paper/2018/hash/2ecd2bd94734e5dd392d8678bc64cdab-Abstract.html

Shiqi Wang, Huan Zhang, Kaidi Xu, Xue Lin, Suman Jana, Cho-Jui Hsieh, and J Zico Kolter. 2021. Beta-CROWN: Efficient Bound Propagation with Per-neuron Split Constraints for Neural Network Robustness Verification. In NeurIPS (2021). https://proceedings.neurips.cc/paper/2021/hash/fac7fead96dafceaf80c1daffeae82a4-Abstract.html

Zhilu Wang, Chao Huang, and Qi Zhu. 2022a. Efficient Global Robustness Certification of Neural Networks via Interleaving Twin-Network Encoding. In DATE 2022 (2022). https://doi.org/10.24963/IJCAI2023/727

Zhilu Wang, Yixuan Wang, Feisi Fu, Ruochen Jiao, Chao Huang, Wenchao Li, and Qi Zhu. 2022b. A Tool for Neural Network Global Robustness Certification and Training. In https://doi.org/10.48550/ARXIV.2208.07289 2022 (2022). https://doi.org/10.48550/ARXIV.2208.07289
Tong Wu, Liang Tong, and Yevgeniy Vorobeychik. 2019. Defending Against Physically Realizable Attacks on Image Classification. In ICLR (2019). https://openreview.net/forum?id=H1xscnEKDr

Han Xiao, Kashif Rasul, and Roland Vollgraf. 2017. Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms. In http://arxiv.org/abs/1708.07747 (2017). https://doi.org/10.48550/arXiv.1708.07747

Zhuolin Yang, Linyi Li, Xiaojun Xu, Bhavya Kailkhura, Tao Xie, and Bo Li. 2022. On the Certified Robustness for Ensemble Models and Beyond. In ICLR (2022). https://openreview.net/forum?id=tUa4REjGjTf

Bohang Zhang, Du Jiang, Di He, and Liwei Wang. 2022a. Rethinking Lipschitz Neural Networks and Certified Robustness: A Boolean Function Perspective. In NeurIPS (2022). http://papers.nips.cc/paper_files/paper/2022/hash/7b04ec5f2b89d7f601382c422dfe07af-Abstract-Conference.html

Chaoning Zhang, Philipp Benz, Tooba Imtiaz, and In So Kweon. 2020. Understanding Adversarial Examples From the Mutual Influence of Images and Perturbations. In CVPR (2020). https://doi.org/10.48550/arXiv.2007.06189

Huan Zhang, Shiqi Wang, Kaidi Xu, Yihan Wang, Suman Jana, Cho-Jui Hsieh, and J. Zico Kolter. 2022b. A Branch and Bound Framework for Stronger Adversarial Attacks of ReLU Networks. In ICML (2022). https://proceedings.mlr.press/v162/zhang22ae.html

Received 19-OCT-2023; accepted 2024-02-24